{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook: Use NN to predict disease from chemicals using Opa2Vec vectors\n",
    "<b> Author: </b> Ian Coleman <br>\n",
    "<b> Purpose: </b> Take the vectors created in the opa2vec notebook. This took chemical go functions\n",
    "    and disease go function, creating vectors for each. Train a NN to predict positive chem-dis relationships from these vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from pandas_ml import ConfusionMatrix\n",
    "import json\n",
    "import subprocess\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "#Set random seed\n",
    "np.random.seed(1606)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mem_usage(pandas_obj):\n",
    "    if isinstance(pandas_obj,pd.DataFrame):\n",
    "        usage_b = pandas_obj.memory_usage(deep=True).sum()\n",
    "    else: # we assume if not a df it's a series\n",
    "        usage_b = pandas_obj.memory_usage(deep=True)\n",
    "    usage_mb = usage_b / 1024 ** 2 # convert bytes to megabytes\n",
    "    return \"{:03.2f} MB\".format(usage_mb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Vectors and Pre-Process them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Gofunc vec file\n",
    "with open('go-gofuncs.lst', 'r') as file:\n",
    "    text = file.read()\n",
    "    \n",
    "# Strip and split vector data into list of lists [chem, vec]\n",
    "text = text.replace('\\n', '')\n",
    "text = text.split(']')\n",
    "text = [item.strip().split(' [') for item in text]\n",
    "\n",
    "# Turn it into a data frame\n",
    "df = pd.DataFrame(text)\n",
    "df.columns = ['ID', 'Vector']\n",
    "\n",
    "# Clean\n",
    "df = df.dropna()\n",
    "df['Vector'] = df.Vector.map(lambda x: x.rstrip().lstrip().replace('    ', ' ').replace('   ', ' ').replace('  ', ' ').replace(' ', ','))\n",
    "\n",
    "# Turn vector column into a list\n",
    "df['Vector'] = df.Vector.map(lambda x: x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MESH:D012559</td>\n",
       "      <td>[-0.14418675, 0.00148405, -0.466548, -0.183814...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MESH:D009404</td>\n",
       "      <td>[0.00648174, 0.12445401, 0.00781578, -0.099749...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MESH:D001749</td>\n",
       "      <td>[-0.24630277, 0.03765613, -0.37198013, -0.1262...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MESH:D011471</td>\n",
       "      <td>[-0.14594647, 0.10092776, -0.7002952, -0.17298...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MESH:D008106</td>\n",
       "      <td>[-0.30239132, 0.1475905, -0.58684784, -0.19144...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID                                             Vector\n",
       "0  MESH:D012559  [-0.14418675, 0.00148405, -0.466548, -0.183814...\n",
       "1  MESH:D009404  [0.00648174, 0.12445401, 0.00781578, -0.099749...\n",
       "2  MESH:D001749  [-0.24630277, 0.03765613, -0.37198013, -0.1262...\n",
       "3  MESH:D011471  [-0.14594647, 0.10092776, -0.7002952, -0.17298...\n",
       "4  MESH:D008106  [-0.30239132, 0.1475905, -0.58684784, -0.19144..."
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create DF for NN\n",
    "Munge the df into the following columns:<br>\n",
    "ChemID DisID ChemVec DisVec PositiveAssociationExists(binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ChemicalID</th>\n",
       "      <th>DiseaseID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C112297</td>\n",
       "      <td>MESH:D006948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C112297</td>\n",
       "      <td>MESH:D012640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C425777</td>\n",
       "      <td>MESH:D006948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C013567</td>\n",
       "      <td>MESH:D006333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C418863</td>\n",
       "      <td>MESH:D013262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ChemicalID     DiseaseID\n",
       "0    C112297  MESH:D006948\n",
       "1    C112297  MESH:D012640\n",
       "2    C425777  MESH:D006948\n",
       "3    C013567  MESH:D006333\n",
       "4    C418863  MESH:D013262"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Import file of proven chem-dis positive associations (created in ctd-to-nt notebook from ctd data)\n",
    "chem_dis = pd.read_csv('../ctd-to-nt/chem-dis-pos-assocs.csv')\n",
    "chem_dis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of any chems/diseases that don't have a vector\n",
    "chem_dis['DiseaseID'] = chem_dis['DiseaseID'].astype(str)\n",
    "df['ID'] = df['ID'].astype(str)\n",
    "id_list = df.ID.tolist() # list of chems+diseases with vecs\n",
    "\n",
    "chem_dis['hasDVec'] = chem_dis.DiseaseID.map(lambda x: x in id_list)\n",
    "chem_dis['hasCVec'] = chem_dis.ChemicalID.map(lambda x: x in id_list)\n",
    "chem_dis = chem_dis.loc[(chem_dis['hasDVec'] == True) & (chem_dis['hasCVec'] == True)]\n",
    "chem_dis = chem_dis.drop(['hasDVec','hasCVec'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all info into one df\n",
    "# this df now contains only correlated diseases and vecs\n",
    "df_d = df.copy()\n",
    "df_d.columns= ['DiseaseID', 'DVec']\n",
    "df_c = df.copy()\n",
    "df_c.columns= ['ChemicalID', 'CVec']\n",
    "df1 = pd.merge(chem_dis, df_d, on='DiseaseID')\n",
    "df1 = pd.merge(df1, df_c, on='ChemicalID')\n",
    "\n",
    "df1['Correlation'] = 1 # currently only have correlated in there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ChemicalID</th>\n",
       "      <th>DiseaseID</th>\n",
       "      <th>DVec</th>\n",
       "      <th>CVec</th>\n",
       "      <th>Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C028474</td>\n",
       "      <td>MESH:D006528</td>\n",
       "      <td>[-1.79790407e-01, 2.88105961e-02, -9.71369967e...</td>\n",
       "      <td>[-0.04686837, 0.11625388, 0.01462597, -0.10324...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C028474</td>\n",
       "      <td>MESH:D005355</td>\n",
       "      <td>[-0.01172249, 0.13109156, 0.00888951, -0.15944...</td>\n",
       "      <td>[-0.04686837, 0.11625388, 0.01462597, -0.10324...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C028474</td>\n",
       "      <td>MESH:D006529</td>\n",
       "      <td>[-0.10915242, 0.09243832, -0.04729906, -0.1477...</td>\n",
       "      <td>[-0.04686837, 0.11625388, 0.01462597, -0.10324...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C028474</td>\n",
       "      <td>MESH:D006965</td>\n",
       "      <td>[-2.88997646e-02, 1.38664156e-01, 3.66541967e-...</td>\n",
       "      <td>[-0.04686837, 0.11625388, 0.01462597, -0.10324...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C028474</td>\n",
       "      <td>MESH:D008114</td>\n",
       "      <td>[-0.17260571, 0.03253803, -0.18018836, -0.0771...</td>\n",
       "      <td>[-0.04686837, 0.11625388, 0.01462597, -0.10324...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ChemicalID     DiseaseID                                               DVec  \\\n",
       "0    C028474  MESH:D006528  [-1.79790407e-01, 2.88105961e-02, -9.71369967e...   \n",
       "1    C028474  MESH:D005355  [-0.01172249, 0.13109156, 0.00888951, -0.15944...   \n",
       "2    C028474  MESH:D006529  [-0.10915242, 0.09243832, -0.04729906, -0.1477...   \n",
       "3    C028474  MESH:D006965  [-2.88997646e-02, 1.38664156e-01, 3.66541967e-...   \n",
       "4    C028474  MESH:D008114  [-0.17260571, 0.03253803, -0.18018836, -0.0771...   \n",
       "\n",
       "                                                CVec  Correlation  \n",
       "0  [-0.04686837, 0.11625388, 0.01462597, -0.10324...            1  \n",
       "1  [-0.04686837, 0.11625388, 0.01462597, -0.10324...            1  \n",
       "2  [-0.04686837, 0.11625388, 0.01462597, -0.10324...            1  \n",
       "3  [-0.04686837, 0.11625388, 0.01462597, -0.10324...            1  \n",
       "4  [-0.04686837, 0.11625388, 0.01462597, -0.10324...            1  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add control (uncorrelated) rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3153"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8920, 2)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chem_dis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate dfs of dis-vecs and chem-vecs ( in order to generate additional rows for df1)\n",
    "dis = df.ID.map(lambda x: ('MESH' in x) | ('OMIM' in x))\n",
    "chems = df.ID.map(lambda x: ('MESH' not in x) & ('OMIM' not in x))\n",
    "\n",
    "df_chems = df[chems]\n",
    "df_dis = df[dis]\n",
    "df_chems = df_chems.reset_index(drop=True)\n",
    "df_dis = df_dis.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  8919\n",
      "(17839, 5)\n",
      "(17675, 5)\n"
     ]
    }
   ],
   "source": [
    "# Add unrelated pairs to df1\n",
    "no_rows = (df1.shape[0]-1)   # This is a parameter to be tuned --> how many uncorrelated pairs do we want\n",
    "print('shape: ', no_rows)\n",
    "\n",
    "# Randomly select chems and diseases (as many as there are related pairs)\n",
    "no_chems = len(df_chems) -1\n",
    "no_dis = len(df_dis) -1\n",
    "rand_chems = np.random.choice(no_chems, no_rows, replace=True)\n",
    "rand_dis = np.random.choice(no_dis, no_rows, replace=True)\n",
    "\n",
    "# Add the new pairs as rows\n",
    "for x in range(0, no_rows):\n",
    "    int1 = rand_chems[x]\n",
    "    int2 = rand_dis[x]\n",
    "    chem, chemvec = df_chems.loc[int1, 'ID'], df_chems.loc[int1, 'Vector']\n",
    "    dis, disvec = df_dis.loc[int2, 'ID'], df_dis.loc[int2, 'Vector']\n",
    "    df1 = df1.append({'ChemicalID':chem, 'DiseaseID':dis, 'CVec':chemvec, 'DVec':disvec, 'Correlation':0}, ignore_index=True)\n",
    "\n",
    "print(df1.shape)\n",
    "# Drop any duplicates (removes known correlated pairs accidentally generated as uncorrelated)\n",
    "df1 = df1.drop_duplicates(subset=['ChemicalID', 'DiseaseID'], keep=False)\n",
    "print(df1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the elements of the vectors to actual numbers\n",
    "df1['DVec'] = df1.DVec.map(lambda x: [float(i) for i in x])\n",
    "df1['CVec'] = df1.CVec.map(lambda x: [float(i) for i in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Phenotype Vecs\n",
    "Got a list of Chem-Phenotypes from Sara Alth, where did these come from originally?\n",
    "They have CID identifiers (Pubchem). Need to convert CTD ID to CID ID\n",
    "Use API like so \n",
    "http://pubchem.ncbi.nlm.nih.gov/rest/pug/substance/sourceid/Comparative%20Toxicogenomics%20Database/C533207/cids/TXT/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MESH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MESH:D003920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MESH:D003924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MESH:D008113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MESH:D009369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MESH:D009765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           MESH\n",
       "0  MESH:D003920\n",
       "1  MESH:D003924\n",
       "2  MESH:D008113\n",
       "3  MESH:D009369\n",
       "4  MESH:D009765"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # First Get FULL list of diseases from CTD\n",
    "# diseases = pd.read_csv('../ctd-to-nt/all-diseases-w-genes-ctd.txt', names=['MESH'])\n",
    "# diseases.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1671\n"
     ]
    }
   ],
   "source": [
    "# # Then use premade map to get DOIDs for MESHs\n",
    "# mapper = pd.read_csv('chem_dis_to_CID_DOID.csv')\n",
    "# print(mapper.DOID.nunique()) # 1671\n",
    "# mesh_to_doid = dict(zip(mapper.ID, mapper.DOID))\n",
    "# diseases['DOID'] = diseases.MESH.map(lambda x: mesh_to_doid.get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['DOID'] = df1.DiseaseID.map(lambda x: mesh_to_doid.get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Then get phens for all DOIDs\n",
    "# # Read in the association file from Sara\n",
    "# dis_phens = pd.read_csv('../opa/Disease-PhenotypeAssocation.txt', sep=' ', names=['DOID', 'Phenotype'])\n",
    "\n",
    "# # Let's create one associations file for each ontology (I believe we need to run HP and MP separately)\n",
    "# dis_phens.columns = ['ID', 'Phen']\n",
    "\n",
    "# # Split into MP/HP\n",
    "# total_hp = dis_phens.Phen.map(lambda x: 'obolibrary.org/obo/HP' in str(x))\n",
    "# total_mp = dis_phens.Phen.map(lambda x: 'obolibrary.org/obo/MP' in str(x))\n",
    "\n",
    "# total_hp = dis_phens[total_hp]\n",
    "# total_mp = dis_phens[total_mp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num HP associations:  193000\n",
      "Num MP associations:  88633\n",
      "Num ents with MP phen assocs:  7536\n",
      "Num ents with HP phen assocs:  7710\n"
     ]
    }
   ],
   "source": [
    "# # Export the association files and print some stats (more stats later)\n",
    "# np.savetxt(r'associations_hp.txt', total_hp.values, fmt='%s')\n",
    "# np.savetxt(r'associations_mp.txt', total_mp.values, fmt='%s')\n",
    "\n",
    "# print('Num HP associations: ', total_hp.shape[0])\n",
    "# print('Num MP associations: ', total_mp.shape[0])\n",
    "# print('Num ents with MP phen assocs: ', len(total_mp.ID.unique()))\n",
    "# print('Num ents with HP phen assocs: ', len(total_hp.ID.unique()))\n",
    "\n",
    "# # Create entities lists to inform opa2vec which entities we want vectors for\n",
    "# entities = total_mp.ID.unique().tolist()\n",
    "# np.savetxt(r'entities_mp.lst', entities, fmt='%s')\n",
    "\n",
    "# entities = total_hp.ID.unique().tolist()\n",
    "# np.savetxt(r'entities_hp.lst', entities, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # HP --> takes ages so commenting out\n",
    "# subprocess.check_output('(cd ../../opa2vec/ ; python2 runOPA2Vec.py -ontology ../ontologies/hp.owl -associations ../msc-thesis/opa/associations_hp.txt -entities ../msc-thesis/opa/entities_hp.lst -outfile ../msc-thesis/opa/hpvecs.lst)', shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # MP --> takes ages so commenting out\n",
    "# subprocess.check_output('(cd ../../opa2vec/ ; python2 runOPA2Vec.py -ontology ../ontologies/mp.owl -associations ../msc-thesis/opa/associations_mp.txt -entities ../msc-thesis/opa/entities_mp.lst -outfile ../msc-thesis/opa/mpvecs.lst)', shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This cell and commented ones below ran opa on chem/dis phens but only those chems/dis in df\n",
    "\n",
    "# ## First we'll add DOIDs for diseases and CIDs for chemicals as an intermediate for adding phenotypes\n",
    "# # Read in CSV mapping chems to CID and dis to DOID --> This file is created by phens-opa2vec.ipynb\n",
    "# # mapper = pd.read_csv('entities.lst')\n",
    "# mapper = pd.read_csv('chem_dis_to_CID_DOID.csv')\n",
    "\n",
    "# # Make the maps from this\n",
    "# dis_map = dict(zip(mapper.ID, mapper.DOID))\n",
    "# chem_map = dict(zip(mapper.ID, mapper.CID))\n",
    "\n",
    "# # Apply the maps to df1\n",
    "# df1['DOID'] = df1.DiseaseID.map(lambda x: dis_map.get(x))\n",
    "# df1['CID'] = df1.ChemicalID.map(lambda x: chem_map.get(x))\n",
    "\n",
    "# # Switch None to NaN\n",
    "# df1['DOID'] = df1.DOID.map(lambda x: np.nan if x is None else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read in the association files from Sara\n",
    "# dis_phens = pd.read_csv('Disease-PhenotypeAssocation.txt', sep=' ', names=['DOID', 'Phenotype'])\n",
    "# chem_phens =  pd.read_csv('Drug-PhenotypeAssocation.txt', sep=' ', names=['CID', 'Phenotype'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The association files have a different format for each ID system than mine, homogenise these\n",
    "# # first chem\n",
    "# def cid_standardiser (cid):\n",
    "#     # Must be format CID + 9 int chars, starting with 1 seemingly\n",
    "#     cid = int(cid)\n",
    "#     output = 'CID1' + '0' * (8 - len(str(cid))) + str(cid)\n",
    "#     return output\n",
    "\n",
    "# df1['CID'] = df1.CID.map(lambda x: np.nan if math.isnan(x) else cid_standardiser(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export the chemical list, this will be used to ensure validation test sets don't overlap with this db\n",
    "# np.savetxt(r'chemsInNN.txt', df1.CID, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sider_mod.ChemicalID.unique()# and now disease \n",
    "# def doid_standardiser (doid):\n",
    "#     # Must be format DOID_ + ...\n",
    "#     # I'm unsure about how well this is working so print out the DOIDs that don't match\n",
    "#     doid = doid.replace(':', '_')\n",
    "# #     output = 'CID1' + '0' * (8 - len(str(cid))) + str(cid)\n",
    "#     return doid\n",
    "\n",
    "# df1['DOID'] = df1.DOID.map(lambda x: np.nan if isinstance(x, float) else doid_standardiser(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't uncomment, unnceessary\n",
    "# # Now that the DOIDs and CIDs are theoretically standardised we can chuck in the phens to df1\n",
    "\n",
    "# # First though check which DOIDs and CIDs do not match up --> SEEMS OK\n",
    "# test_doids = df1[df1.DOID.map(lambda x: isinstance(x, str))].DOID.tolist()\n",
    "# imported_doids = dis_phens.DOID.tolist()\n",
    "# for item in test_doids:\n",
    "#     if item not in imported_doids:\n",
    "#         print(item)                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's create one associations file for each ontology (I believe we need to run HP and MP separately)\n",
    "# chem_phens.columns = ['ID', 'Phen']\n",
    "# dis_phens.columns = ['ID', 'Phen']\n",
    "\n",
    "# # Maps for Split into MP/HP\n",
    "# hp_df_crit_d = dis_phens.Phen.map(lambda x: 'obolibrary.org/obo/HP' in str(x))\n",
    "# mp_df_crit_d = dis_phens.Phen.map(lambda x: 'obolibrary.org/obo/MP' in str(x))\n",
    "# hp_df_crit_c = chem_phens.Phen.map(lambda x: 'obolibrary.org/obo/HP' in str(x))\n",
    "# mp_df_crit_c = chem_phens.Phen.map(lambda x: 'obolibrary.org/obo/MP' in str(x))\n",
    "\n",
    "# total_hp = chem_phens[hp_df_crit_c].append(dis_phens[hp_df_crit_d], ignore_index=True)\n",
    "# total_mp = chem_phens[mp_df_crit_c].append(dis_phens[mp_df_crit_d], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_mp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export the association files and print some stats (more stats later)\n",
    "# np.savetxt(r'associations_hp.txt', total_hp.values, fmt='%s')\n",
    "# np.savetxt(r'associations_mp.txt', total_mp.values, fmt='%s')\n",
    "\n",
    "# print('Num HP associations: ', total_hp.shape[0])\n",
    "# print('Num MP associations: ', total_mp.shape[0])\n",
    "# print('Num ents with MP phen assocs: ', len(total_mp.ID.unique()))\n",
    "# print('Num ents with HP phen assocs: ', len(total_hp.ID.unique()))\n",
    "# print('Num MP associations for just diseases: ', total_mp[total_mp.ID.map(lambda x: 'CID' not in x)].shape[0])\n",
    "# print('Num HP associations for just diseases: ', total_hp[total_hp.ID.map(lambda x: 'CID' not in x)].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create entities lists to inform opa2vec which entities we want vectors for\n",
    "# entities = total_mp.ID.unique().tolist()\n",
    "# np.savetxt(r'entities_mp.lst', entities, fmt='%s')\n",
    "\n",
    "# entities = total_hp.ID.unique().tolist()\n",
    "# np.savetxt(r'entities_hp.lst', entities, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HP --> takes ages so commenting out\n",
    "# subprocess.check_output('(cd ../../opa2vec/ ; python2 runOPA2Vec.py -ontology ../ontologies/hp.owl -associations ../msc-thesis/opa/associations_hp.txt -entities ../msc-thesis/opa/entities_hp.lst -outfile ../msc-thesis/opa/hpvecs.lst)', shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MP --> takes ages so commenting out\n",
    "# subprocess.check_output('(cd ../../opa2vec/ ; python2 runOPA2Vec.py -ontology ../ontologies/mp.owl -associations ../msc-thesis/opa/associations_mp.txt -entities ../msc-thesis/opa/entities_mp.lst -outfile ../msc-thesis/opa/mpvecs.lst)', shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now import and integrate the vecs (all phen stuff above here should be in its own notebook )\n",
    "# Import vec file\n",
    "with open('hpvecs.lst', 'r') as file:\n",
    "    text = file.read()\n",
    "    \n",
    "# Strip and split vector data into list of lists [chem, vec]\n",
    "text = text.replace('\\n', '')\n",
    "text = text.split(']')\n",
    "text = [item.strip().split(' [') for item in text]\n",
    "\n",
    "# Turn it into a data frame\n",
    "hp_vecs = pd.DataFrame(text)\n",
    "hp_vecs.columns = ['ID', 'Vector']\n",
    "\n",
    "# Clean\n",
    "hp_vecs = hp_vecs.dropna()\n",
    "hp_vecs['Vector'] = hp_vecs.Vector.map(lambda x: x.rstrip().lstrip().replace('    ', ' ').replace('   ', ' ').replace('  ', ' ').replace(' ', ','))\n",
    "\n",
    "# Turn vector column into a list\n",
    "hp_vecs['Vector'] = hp_vecs.Vector.map(lambda x: x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now import and integrate the mp vecs\n",
    "# Import vec file\n",
    "with open('mpvecs.lst', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Strip and split vector data into list of lists [chem, vec]\n",
    "text = text.replace('\\n', '')\n",
    "text = text.split(']')\n",
    "text = [item.strip().split(' [') for item in text]\n",
    "\n",
    "# Turn it into a data frame\n",
    "mp_vecs = pd.DataFrame(text)\n",
    "mp_vecs.columns = ['ID', 'Vector']\n",
    "\n",
    "# Clean\n",
    "mp_vecs = mp_vecs.dropna()\n",
    "mp_vecs['Vector'] = mp_vecs.Vector.map(lambda x: x.rstrip().lstrip().replace('    ', ' ').replace('   ', ' ').replace('  ', ' ').replace(' ', ','))\n",
    "\n",
    "# Turn vector column into a list\n",
    "mp_vecs['Vector'] = mp_vecs.Vector.map(lambda x: x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    DOID_0001816\n",
       "1    DOID_0002116\n",
       "2    DOID_0014667\n",
       "3    DOID_0050009\n",
       "4    DOID_0050012\n",
       "Name: ID, dtype: object"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Right, now I have hp and mp vecs, match them up into df1\n",
    "mp_vecs.ID.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Maps:\n",
    "mp_ent_to_vec = dict(zip(mp_vecs.ID, mp_vecs.Vector))\n",
    "hp_ent_to_vec = dict(zip(hp_vecs.ID, hp_vecs.Vector))\n",
    "\n",
    "# Map entities to vecs (one set of vecs for mp and another for hp)\n",
    "df1['disPhenVecMP'] = df1.DOID.map(lambda x: mp_ent_to_vec.get(x))\n",
    "df1['disPhenVecHP'] = df1.DOID.map(lambda x: hp_ent_to_vec.get(x))\n",
    "\n",
    "# df1['chemPhenVecHP'] = df1.CID.map(lambda x: hp_ent_to_vec.get(x))\n",
    "# df1['chemPhenVecMP'] = df1.CID.map(lambda x: mp_ent_to_vec.get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # export dis-mpVec and dis-hpVec maps\n",
    "# with open('dis_mpVec_map'+ '.pkl', 'wb') as f:\n",
    "#         pickle.dump(mp_ent_to_vec, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "# with open('dis_hpVec_map'+ '.pkl', 'wb') as f:\n",
    "#         pickle.dump(hp_ent_to_vec, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note these numbers include both correlated and uncorrelated pairs\n",
      "Number of rows with gofuncs:  17675\n",
      "Number of rows with dis mp vec:  0\n",
      "Number of rows with dis hp vec:  0\n",
      "Number of rows with no phen vecs:  11145\n",
      "Number of rows with everything 0\n",
      "Number of correlated pairs with phenvecs 3985\n",
      "Number of uncorrelated pairs with phenvecs 7160\n",
      "Number of correlated pairs with everything 0\n",
      "Number of uncorrelated pairs with everything 0\n",
      "total pos corr 8869\n",
      "total neg corr 8806\n"
     ]
    }
   ],
   "source": [
    "## TODO note that I have removed rows without gofuncVecs, maybe now they should be kept\n",
    "# Print Stats\n",
    "print('Note these numbers include both correlated and uncorrelated pairs')\n",
    "print('Number of rows with gofuncs: ', df1.shape[0]) ##NB change this if keeping rows w/o gofunc vecs\n",
    "print('Number of rows with dis mp vec: ', df1[df1.disPhenVecMP.map(lambda x: x is not None)].shape[0])\n",
    "print('Number of rows with dis hp vec: ', df1[df1.disPhenVecHP.map(lambda x: x is not None)].shape[0])\n",
    "# print('Number of rows with chem mp vec: ', df1[df1.chemPhenVecMP.map(lambda x: x is not None)].shape[0])\n",
    "# print('Number of rows with chem hp vec: ', df1[df1.chemPhenVecHP.map(lambda x: x is not None)].shape[0])\n",
    "no_dis_phen_vecs = df1.disPhenVecHP.map(lambda x: x is None) & df1.disPhenVecMP.map(lambda x: x is None)\n",
    "# no_chem_phen_vecs = df1.chemPhenVecHP.map(lambda x: x is None) & df1.chemPhenVecMP.map(lambda x: x is None)\n",
    "# no_phen_vecs = no_dis_phen_vecs & no_chem_phen_vecs\n",
    "print('Number of rows with no phen vecs: ', df1[no_phen_vecs].shape[0])\n",
    "all_dis_phen_vecs = df1.disPhenVecHP.map(lambda x: x is not None) & df1.disPhenVecMP.map(lambda x: x is not None)\n",
    "# all_chem_phen_vecs = df1.chemPhenVecHP.map(lambda x: x is not None) & df1.chemPhenVecMP.map(lambda x: x is not None)\n",
    "# all_vecs = all_dis_phen_vecs & all_chem_phen_vecs\n",
    "all_vecs_pos_corr = all_vecs & df1.Correlation.map(lambda x: x == 1)\n",
    "all_vecs_neg_corr = all_vecs & df1.Correlation.map(lambda x: x == 0)\n",
    "no_phen_vecs_corr = no_phen_vecs & df1.Correlation.map(lambda x: x == 1)\n",
    "no_phen_vecs_uncorr = no_phen_vecs & df1.Correlation.map(lambda x: x == 0)\n",
    "\n",
    "\n",
    "\n",
    "print('Number of rows with everything', df1[all_vecs].shape[0])\n",
    "\n",
    "print('Number of correlated pairs with phenvecs', df1[no_phen_vecs_corr].shape[0])\n",
    "print('Number of uncorrelated pairs with phenvecs', df1[no_phen_vecs_uncorr].shape[0])\n",
    "\n",
    "\n",
    "print('Number of correlated pairs with everything', df1[all_vecs_pos_corr].shape[0])\n",
    "print('Number of uncorrelated pairs with everything', df1[all_vecs_neg_corr].shape[0])\n",
    "\n",
    "print('total pos corr', df1[df1.Correlation.map(lambda x: x == 1)].shape[0])\n",
    "print('total neg corr', df1[df1.Correlation.map(lambda x: x == 0)].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'chemPhenVecHP'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3062\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3063\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3064\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'chemPhenVecHP'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-140-412bea38d02f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'disPhenVecMP'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'disPhenVecHP'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'chemPhenVecHP'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'chemPhenVecMP'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mempty_vec\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2683\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2684\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2685\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2687\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2690\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2692\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2484\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2486\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2487\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   4113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4115\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4116\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4117\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3063\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3064\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3065\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3067\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'chemPhenVecHP'"
     ]
    }
   ],
   "source": [
    "# Add empty vecs for rows that don't have phen vecs\n",
    "empty_vec = [0] * 200\n",
    "\n",
    "for col in ['disPhenVecMP', 'disPhenVecHP', 'chemPhenVecHP', 'chemPhenVecMP']:\n",
    "    df1[col] = df1[col].map(lambda x: empty_vec if x is None else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the Phen vec elements from string to floats\n",
    "df1['disPhenVecHP'] = df1.disPhenVecHP.map(lambda x: [float(i) for i in x])\n",
    "df1['disPhenVecMP'] = df1.disPhenVecMP.map(lambda x: [float(i) for i in x])\n",
    "df1['chemPhenVecHP'] = df1.chemPhenVecHP.map(lambda x: [float(i) for i in x])\n",
    "df1['chemPhenVecMP'] = df1.chemPhenVecMP.map(lambda x: [float(i) for i in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add CHEBI Vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First add CHEBI IDs for each chem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chem_names = df1.ChemicalID.unique().tolist()\n",
    "# np.savetxt(r'chem_names', total_hp.values, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To add chebi first need CID for ALL Chems so I made this comprehensive map:\n",
    "# Load the map from pickle object\n",
    "def load_obj(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "ctd_cid_map = load_obj('ctd_cid_map')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['CID'] = df1.ChemicalID.map(lambda x: ctd_cid_map.get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert CIDS from bytes to strings and export in order to make a map from CID to CHEBI ID\n",
    "df1['CID'] = df1.CID.str.decode(\"utf-8\")\n",
    "np.savetxt(r'allCIDs.txt', df1.CID.unique(), fmt='%s')\n",
    "\n",
    "## NOTE the next step is MANUAL you need to upload this allCIDs.txt to http://cts.fiehnlab.ucdavis.edu/batch \n",
    "# and download it as ctd_chebi.csv to the current folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many chemicals don't have CIDs? Hence don't get chebi vecs\n",
    "print(df1.shape[0])\n",
    "print(df1[df1.CID.map(lambda x: x is not None)].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CSV mapping CID to CHEBI\n",
    "ctdChebdf = pd.read_csv('ctd_chebi.csv')\n",
    "\n",
    "# Turn it into a dict\n",
    "ctd_chebi = dict(zip(ctdChebdf['PubChem CID'], ctdChebdf['ChEBI']))\n",
    "\n",
    "# Map the cids to chebis\n",
    "df1['CHEBI'] = df1.CID.map(lambda x: ctd_chebi.get(x))\n",
    "df1['CHEBI'] = df1.CHEBI.map(lambda x: None if x == 'No result' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now make CHEBI vecs using the CHEBI IDs\n",
    "# First an association file - just linking each chebi to its own chebi entity\n",
    "\n",
    "# add uri col\n",
    "df1['CHEBI_uri'] = df1.dropna(subset=['CHEBI']).CHEBI.map(lambda x: '<http://purl.obolibrary.org/obo/' + x.replace(':', '_') + '>')\n",
    "\n",
    "# export association file from df\n",
    "np.savetxt(r'CHEBIassociations.txt', df1[['ChemicalID', 'CHEBI_uri']].dropna().drop_duplicates().values, fmt='%s')\n",
    "\n",
    "# Now an entities file\n",
    "chems_for_cheb = df1[['ChemicalID', 'CHEBI_uri']].dropna().drop_duplicates().ChemicalID.tolist()\n",
    "np.savetxt(r'CHEBIentities.txt', chems_for_cheb, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commenting this out now that I already have the vectors, you'll need it if you don't have them\n",
    "# # Now run opa2vec on it \n",
    "# subprocess.check_output('(cd ../../opa2vec/ ; python2 runOPA2Vec.py -ontology ../ontologies/chebi.owl -associations ../msc-thesis/opa/CHEBIassociations.txt -entities ../msc-thesis/opa/CHEBIentities.txt -outfile ../msc-thesis/opa/chebi-vecs.lst)', shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Gofunc vec file\n",
    "with open('chebi-vecs.lst', 'r') as file:\n",
    "    text = file.read()\n",
    "    \n",
    "# Strip and split vector data into list of lists [chem, vec]\n",
    "text = text.replace('\\n', '')\n",
    "text = text.split(']')\n",
    "text = [item.strip().split(' [') for item in text]\n",
    "\n",
    "# Turn it into a data frame\n",
    "df = pd.DataFrame(text)\n",
    "df.columns = ['ID', 'Vector']\n",
    "\n",
    "# Clean\n",
    "df = df.dropna()\n",
    "df['Vector'] = df.Vector.map(lambda x: x.rstrip().lstrip().replace('    ', ' ').replace('   ', ' ').replace('  ', ' ').replace(' ', ','))\n",
    "\n",
    "# Turn vector column into a list\n",
    "df['Vector'] = df.Vector.map(lambda x: x.split(','))\n",
    "\n",
    "# Make a map of it (ChemID to CHEBIvec)\n",
    "chem_to_chebi_vec = dict(zip(df.ID, df.Vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['CHEBIvec'] = df1.ChemicalID.map(lambda x: chem_to_chebi_vec.get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many rows have CHEBI Vecs?\n",
    "print('Total Rows', df1.shape[0])\n",
    "print('Total Correlated Rows', df1[df1.Correlation == 1].shape[0])\n",
    "print('Total CHEBI vec rows', df1.dropna(subset=['CHEBIvec']).shape[0])\n",
    "print('Total CHEBI vec correlated rows', df1[df1.Correlation == 1].dropna(subset=['CHEBIvec']).shape[0])\n",
    "print('Total Chems', len(df1.ChemicalID.unique()))\n",
    "print('Total Chems with CHEBI Vec', len(df1.dropna(subset=['CHEBIvec']).ChemicalID.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add empty vecs for rows that don't have Chebi vecs\n",
    "empty_vec = [0] * 200\n",
    "df1['CHEBIvec'] = df1['CHEBIvec'].map(lambda x: empty_vec if x is None else x)\n",
    "\n",
    "# Change the Chebi vec elements from string to floats\n",
    "df1['CHEBIvec'] = df1.CHEBIvec.map(lambda x: [float(i) for i in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add DO Vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Gofunc vec file\n",
    "with open('do-vecs.lst', 'r') as file:\n",
    "    text = file.read()\n",
    "    \n",
    "# Strip and split vector data into list of lists [disease, vec]\n",
    "text = text.replace('\\n', '')\n",
    "text = text.split(']')\n",
    "text = [item.strip().split(' [') for item in text]\n",
    "\n",
    "# Turn it into a data frame\n",
    "df = pd.DataFrame(text)\n",
    "df.columns = ['ID', 'Vector']\n",
    "\n",
    "# Clean\n",
    "df = df.dropna()\n",
    "df['Vector'] = df.Vector.map(lambda x: x.rstrip().lstrip().replace('    ', ' ').replace('   ', ' ').replace('  ', ' ').replace(' ', ','))\n",
    "\n",
    "# Turn vector column into a list\n",
    "df['Vector'] = df.Vector.map(lambda x: x.split(','))\n",
    "\n",
    "# Make a map of it (DisID to DOvec)\n",
    "dis_to_DOvec = dict(zip(df.ID, df.Vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['DOvec'] = df1.DiseaseID.map(lambda x: dis_to_DOvec.get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df1[df1.DOvec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the DO vec elements from string to floats\n",
    "df1['DOvec'] = df1.DOvec.map(lambda x: [float(i) for i in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADD HINO vecs\n",
    "This file is getting a bit long "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import HINO vec file\n",
    "with open('hinoVecs.lst', 'r') as file:\n",
    "    text = file.read()\n",
    "    \n",
    "# Strip and split vector data into list of lists [chem, vec]\n",
    "text = text.replace('\\n', '')\n",
    "text = text.split(']')\n",
    "text = [item.strip().split(' [') for item in text]\n",
    "\n",
    "# Turn it into a data frame\n",
    "df = pd.DataFrame(text)\n",
    "df.columns = ['ID', 'Vector']\n",
    "\n",
    "# Clean\n",
    "df = df.dropna()\n",
    "df['Vector'] = df.Vector.map(lambda x: x.rstrip().lstrip().replace('    ', ' ').replace('   ', ' ').replace('  ', ' ').replace(' ', ','))\n",
    "\n",
    "# Turn vector column into a list\n",
    "df['Vector'] = df.Vector.map(lambda x: x.split(','))\n",
    "\n",
    "# Make a map of it (DisID to DOvec)\n",
    "entity_to_HINOvec = dict(zip(df.ID, df.Vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['dis_HINOvec'] = df1.DiseaseID.map(lambda x: entity_to_HINOvec.get(x))\n",
    "df1['chem_HINOvec'] = df1.ChemicalID.map(lambda x: entity_to_HINOvec.get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('HINO dis vecs: ', df1[df1.dis_HINOvec.map(lambda x: x is not None)].shape[0])\n",
    "print('HINO chem vecs: ', df1[df1.chem_HINOvec.map(lambda x: x is not None)].shape[0])\n",
    "at_least_one = df1.chem_HINOvec.map(lambda x: x is not None) | df1.dis_HINOvec.map(lambda x: x is not None)\n",
    "print('At least one hino vec: ', df1[at_least_one].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add empty vecs in place of None\n",
    "empty_vec = [0] * 200\n",
    "\n",
    "for col in ['dis_HINOvec', 'chem_HINOvec']:\n",
    "    df1[col] = df1[col].map(lambda x: empty_vec if x is None else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the HINO vec elements from string to floats\n",
    "df1['dis_HINOvec'] = df1.dis_HINOvec.map(lambda x: [float(i) for i in x])\n",
    "df1['chem_HINOvec'] = df1.chem_HINOvec.map(lambda x: [float(i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocess\n",
    "Now that we have the df ready, let's split it into train/test/validation sets and convert it into numpy arrays so it can be consumed by a Keras NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1[df1.Correlation == 1].shape[0])\n",
    "print(df1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Version for phen and gofunc vecs\n",
    "# # For Keras, need to turn inputs into numpy arrays instead of pandas df\n",
    "# # First create single np array of all vecs... not pretty:\n",
    "# Dvecs = pd.DataFrame(df1.DVec.values.tolist(), index= df1.index)\n",
    "# Cvecs = pd.DataFrame(df1.CVec.values.tolist(), index= df1.index)\n",
    "# gofuncs = Dvecs.merge(Cvecs, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "# DMPvecs = pd.DataFrame(df1.disPhenVecHP.values.tolist(), index= df1.index)\n",
    "# DHPvecs = pd.DataFrame(df1.disPhenVecMP.values.tolist(), index= df1.index)\n",
    "# disPvecs = DMPvecs.merge(DHPvecs, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "# CMPvecs = pd.DataFrame(df1.chemPhenVecHP.values.tolist(), index= df1.index)\n",
    "# CHPvecs = pd.DataFrame(df1.chemPhenVecMP.values.tolist(), index= df1.index)\n",
    "# chemPvecs = CMPvecs.merge(CHPvecs, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "# phenVecs = disPvecs.merge(chemPvecs, how='outer', left_index=True, right_index=True)\n",
    "# all_X = phenVecs.merge(gofuncs, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "# all_X = np.array(all_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version for just gofunc vecs\n",
    "# For Keras, need to turn inputs into numpy arrays instead of pandas df\n",
    "# First create single np array of all vecs... not pretty:\n",
    "Dvecs = pd.DataFrame(df1.DVec.values.tolist(), index= df1.index)\n",
    "Cvecs = pd.DataFrame(df1.CVec.values.tolist(), index= df1.index)\n",
    "all_X = Dvecs.merge(Cvecs, how='outer', left_index=True, right_index=True)\n",
    "all_X = np.array(all_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Version for Chebi vecs with gofuncs\n",
    "# # For Keras, need to turn inputs into numpy arrays instead of pandas df\n",
    "# # First create single np array of all vecs... not pretty:\n",
    "# Dvecs = pd.DataFrame(df1.DVec.values.tolist(), index= df1.index)\n",
    "# Cvecs = pd.DataFrame(df1.CVec.values.tolist(), index= df1.index)\n",
    "# CHEBvecs = pd.DataFrame(df1.CHEBIvec.values.tolist(), index = df1.index)\n",
    "# gofuncs = Dvecs.merge(Cvecs, how='outer', left_index=True, right_index=True)\n",
    "# all_X = gofuncs.merge(CHEBvecs, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "# all_X = np.array(all_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ## Version for just CHEBI vecs\n",
    "# # For Keras, need to turn inputs into numpy arrays instead of pandas df\n",
    "# # First create single np array of all vecs... not pretty:\n",
    "# CHEBvecs = pd.DataFrame(df1.CHEBIvec.values.tolist(), index = df1.index)\n",
    "# all_X = np.array(CHEBvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Version for GoFuncs, Phens, CHEBI\n",
    "# # For Keras, need to turn inputs into numpy arrays instead of pandas df\n",
    "# # First create single np array of all vecs... not pretty:\n",
    "# Dvecs = pd.DataFrame(df1.DVec.values.tolist(), index= df1.index)\n",
    "# Cvecs = pd.DataFrame(df1.CVec.values.tolist(), index= df1.index)\n",
    "# gofuncs = Dvecs.merge(Cvecs, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "# DMPvecs = pd.DataFrame(df1.disPhenVecHP.values.tolist(), index= df1.index)\n",
    "# DHPvecs = pd.DataFrame(df1.disPhenVecMP.values.tolist(), index= df1.index)\n",
    "# disPvecs = DMPvecs.merge(DHPvecs, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "# CMPvecs = pd.DataFrame(df1.chemPhenVecHP.values.tolist(), index= df1.index)\n",
    "# CHPvecs = pd.DataFrame(df1.chemPhenVecMP.values.tolist(), index= df1.index)\n",
    "# chemPvecs = CMPvecs.merge(CHPvecs, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "# phenVecs = disPvecs.merge(chemPvecs, how='outer', left_index=True, right_index=True)\n",
    "# all_X = phenVecs.merge(gofuncs, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "# CHEBvecs = pd.DataFrame(df1.CHEBIvec.values.tolist(), index = df1.index)\n",
    "# all_X = CHEBvecs.merge(all_X, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "# all_X = np.array(all_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Version for GoFuncs, Phens, CHEBI, DO incl chem phen\n",
    "# # For Keras, need to turn inputs into numpy arrays instead of pandas df\n",
    "# # First create single np array of all vecs... not pretty:\n",
    "# Dvecs = pd.DataFrame(df1.DVec.values.tolist(), index= df1.index)\n",
    "# Cvecs = pd.DataFrame(df1.CVec.values.tolist(), index= df1.index)\n",
    "# gofuncs = Dvecs.merge(Cvecs, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "# DMPvecs = pd.DataFrame(df1.disPhenVecHP.values.tolist(), index= df1.index)\n",
    "# DHPvecs = pd.DataFrame(df1.disPhenVecMP.values.tolist(), index= df1.index)\n",
    "# disPvecs = DMPvecs.merge(DHPvecs, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "# CMPvecs = pd.DataFrame(df1.chemPhenVecHP.values.tolist(), index= df1.index)\n",
    "# CHPvecs = pd.DataFrame(df1.chemPhenVecMP.values.tolist(), index= df1.index)\n",
    "# chemPvecs = CMPvecs.merge(CHPvecs, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "# phenVecs = disPvecs.merge(chemPvecs, how='outer', left_index=True, right_index=True)\n",
    "# all_X = phenVecs.merge(gofuncs, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "# CHEBvecs = pd.DataFrame(df1.CHEBIvec.values.tolist(), index = df1.index)\n",
    "# all_X = CHEBvecs.merge(all_X, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "# DOvecs = pd.DataFrame(df1.DOvec.values.tolist(), index = df1.index)\n",
    "# all_X = DOvecs.merge(all_X, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "# all_X = np.array(all_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Version for GoFuncs, Phens, CHEBI, DO EXCL chem phen\n",
    "# # For Keras, need to turn inputs into numpy arrays instead of pandas df\n",
    "# # First create single np array of all vecs... not pretty:\n",
    "# Dvecs = pd.DataFrame(df1.DVec.values.tolist(), index= df1.index)\n",
    "# Cvecs = pd.DataFrame(df1.CVec.values.tolist(), index= df1.index)\n",
    "# gofuncs = Dvecs.merge(Cvecs, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "# DMPvecs = pd.DataFrame(df1.disPhenVecHP.values.tolist(), index= df1.index)\n",
    "# DHPvecs = pd.DataFrame(df1.disPhenVecMP.values.tolist(), index= df1.index)\n",
    "# disPvecs = DMPvecs.merge(DHPvecs, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "# all_X = disPvecs.merge(gofuncs, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "# CHEBvecs = pd.DataFrame(df1.CHEBIvec.values.tolist(), index = df1.index)\n",
    "# all_X = CHEBvecs.merge(all_X, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "# DOvecs = pd.DataFrame(df1.DOvec.values.tolist(), index = df1.index)\n",
    "# all_X = DOvecs.merge(all_X, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "# all_X = np.array(all_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Version for GoFuncs, Phens, CHEBI, DO, HINO, -- EXCL chem phen\n",
    "# # For Keras, need to turn inputs into numpy arrays instead of pandas df\n",
    "# # First create single np array of all vecs... not pretty:\n",
    "# Dvecs = pd.DataFrame(df1.DVec.values.tolist(), index= df1.index)\n",
    "# Cvecs = pd.DataFrame(df1.CVec.values.tolist(), index= df1.index)\n",
    "# gofuncs = Dvecs.merge(Cvecs, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "# DMPvecs = pd.DataFrame(df1.disPhenVecHP.values.tolist(), index= df1.index)\n",
    "# DHPvecs = pd.DataFrame(df1.disPhenVecMP.values.tolist(), index= df1.index)\n",
    "# disPvecs = DMPvecs.merge(DHPvecs, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "# all_X = disPvecs.merge(gofuncs, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "# CHEBvecs = pd.DataFrame(df1.CHEBIvec.values.tolist(), index = df1.index)\n",
    "# all_X = CHEBvecs.merge(all_X, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "# DOvecs = pd.DataFrame(df1.DOvec.values.tolist(), index = df1.index)\n",
    "# all_X = DOvecs.merge(all_X, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "# dHINOvecs = pd.DataFrame(df1.dis_HINOvec.values.tolist(), index=df1.index)\n",
    "# cHINOvecs = pd.DataFrame(df1.chem_HINOvec.values.tolist(), index=df1.index)\n",
    "# hinovecs = cHINOvecs.merge(dHINOvecs, how='outer', left_index=True, right_index=True)\n",
    "# all_X = all_X.merge(hinovecs, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "# all_X = np.array(all_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create np array of the y output\n",
    "all_y = np.array(df1.Correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('y shape: ', all_y.shape)\n",
    "print('X shape: ', all_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create training, test, val set in a way that we can later look at the rows of each BY ROWS\n",
    "# total_rows = len(all_X)\n",
    "# row_numbers = list(range(0, total_rows))\n",
    "\n",
    "# training_rows = random.sample(row_numbers, int(round(total_rows * .6)))\n",
    "# row_numbers = set(row_numbers) - set(training_rows)\n",
    "\n",
    "# test_rows = random.sample(row_numbers, int(round(total_rows * .2)))\n",
    "# row_numbers = set(row_numbers) - set(test_rows)\n",
    "\n",
    "# val_rows = list(row_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train, test, val BY CHEMICAL instead of by random rows\n",
    "print('number of chemicals: ', len(df1.ChemicalID.unique()))\n",
    "print('number of dis: ', len(df1.DiseaseID.unique()))\n",
    "chems = list(df1.ChemicalID.unique())\n",
    "random.shuffle(chems)\n",
    "\n",
    "total_chems = len(chems)\n",
    "train_chems = chems[:round(total_chems * .6)]\n",
    "test_chems = chems[round(total_chems * .6):round(total_chems * .8)]\n",
    "val_chems = chems[round(total_chems * .8):]\n",
    "\n",
    "print(len(train_chems), len(test_chems), len(val_chems))\n",
    "\n",
    "# Now get the row numbers for each set of chemicals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export the chemical list, this will be used to ensure validation test sets don't overlap with this db\n",
    "# np.savetxt(r'chemsInNN.txt', chems, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['train'] = df1.ChemicalID.isin(train_chems)\n",
    "df1['test'] = df1.ChemicalID.isin(test_chems)\n",
    "df1['val'] = df1.ChemicalID.isin(val_chems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split by CHEMICAL instead of by random rows\n",
    "print('number of chemicals: ', len(df1.ChemicalID.unique()))\n",
    "print('number of dis: ', len(df1.DiseaseID.unique()))\n",
    "chems = list(df1.ChemicalID.unique())\n",
    "df1 = df1.reset_index()\n",
    "training_rows = df1.index[df1.train == True].tolist()\n",
    "test_rows = df1.index[df1.test == True].tolist()\n",
    "val_rows = df1.index[df1.val == True].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train, test, val\n",
    "X_train, X_test, X_val = all_X[training_rows], all_X[test_rows], all_X[val_rows]\n",
    "y_train, y_test, y_val = all_y[training_rows], all_y[test_rows], all_y[val_rows]\n",
    "\n",
    "print(len(training_rows), len(test_rows), len(val_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split into train, test, val --> OLD WAY\n",
    "# X_train, X_test, y_train, y_test = train_test_split(all_X, all_y, test_size=0.2, random_state=1606)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1606)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Establish NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Establish the model architecture\n",
    "#it's safe to say that I don't know what I'm doing here\n",
    "model = keras.Sequential([\n",
    "#     keras.layers.Dense(400, activation=tf.nn.relu), \n",
    "    keras.layers.Flatten(input_shape=[400]),\n",
    "    keras.layers.Dense(200, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(60, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(10, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Compile the model (give it loss func, optimise func and eval metric)\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(), # determines how the model is adapted based on loss func\n",
    "              loss='binary_crossentropy', # measure of accuracy during training\n",
    "              metrics=['accuracy']) # measure for train and testing steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-training, set up training params\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Train\n",
    "model.fit(X_train, y_train, epochs=60, validation_data=(X_val, y_val) ) #, callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ...and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Evaluate\n",
    "# Accuracy\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get actual predictions for test set\n",
    "predictions = model.predict(X_test)\n",
    "rounded_predictions = [int(float(round(x[0]))) for x in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Confusion Matrix\n",
    "\n",
    "confusion_matrix = ConfusionMatrix(y_test, rounded_predictions)\n",
    "print(\"Confusion matrix:\\n%s\" % confusion_matrix)\n",
    "confusion_matrix.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC AUC\n",
    "print('ROC AUC: ', roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Sider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sider_mod = pd.read_csv('../validation/Sider_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Need to turn all to float\n",
    "# import ast\n",
    "\n",
    "# all_vecs = ['ChemGoVec', 'DisGoVec', 'disPhenVecMP', 'disPhenVecHP', 'CHEBIvec', 'DOvec', \n",
    "#             'dis_HINOvec', 'chem_HINOvec', ]\n",
    "\n",
    "# for col in all_vecs:\n",
    "#     sider_mod[col] = [ast.literal_eval(x) for x in sider_mod[col]]\n",
    "#     sider_mod[col] = sider_mod[col].map(lambda x: [float(i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For Keras, need to turn inputs into numpy arrays instead of pandas df\n",
    "# # First create single np array of all vecs... not pretty:\n",
    "# Dvecs = pd.DataFrame(sider_mod.DisGoVec.values.tolist(), index= sider_mod.index)\n",
    "# Cvecs = pd.DataFrame(sider_mod.ChemGoVec.values.tolist(), index= sider_mod.index)\n",
    "# gofuncs = Dvecs.merge(Cvecs, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "# DMPvecs = pd.DataFrame(sider_mod.disPhenVecHP.values.tolist(), index= sider_mod.index)\n",
    "# DHPvecs = pd.DataFrame(sider_mod.disPhenVecMP.values.tolist(), index= sider_mod.index)\n",
    "# disPvecs = DMPvecs.merge(DHPvecs, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "# all_X = disPvecs.merge(gofuncs, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "# CHEBvecs = pd.DataFrame(sider_mod.CHEBIvec.values.tolist(), index = sider_mod.index)\n",
    "# all_X = CHEBvecs.merge(all_X, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "# DOvecs = pd.DataFrame(sider_mod.DOvec.values.tolist(), index = sider_mod.index)\n",
    "# all_X = DOvecs.merge(all_X, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "# dHINOvecs = pd.DataFrame(sider_mod.dis_HINOvec.values.tolist(), index=sider_mod.index)\n",
    "# cHINOvecs = pd.DataFrame(sider_mod.chem_HINOvec.values.tolist(), index=sider_mod.index)\n",
    "# hinovecs = cHINOvecs.merge(dHINOvecs, how='outer', left_index=True, right_index=True)\n",
    "# all_X = all_X.merge(hinovecs, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "# all_X = np.array(all_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version for just gofunc vecs\n",
    "# For Keras, need to turn inputs into numpy arrays instead of pandas df\n",
    "# First create single np array of all vecs... not pretty:\n",
    "\n",
    "Dvecs = pd.DataFrame(sider_mod.DisGoVec.values.tolist(), index= sider_mod.index)\n",
    "Cvecs = pd.DataFrame(sider_mod.ChemGoVec.values.tolist(), index= sider_mod.index)\n",
    "# Dvecs = pd.DataFrame(df1.DVec.values.tolist(), index= df1.index)\n",
    "# Cvecs = pd.DataFrame(df1.CVec.values.tolist(), index= df1.index)\n",
    "all_X = Dvecs.merge(Cvecs, how='outer', left_index=True, right_index=True)\n",
    "all_X = np.array(all_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create np array of the y output\n",
    "all_y = np.array(sider_mod.Correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('y shape: ', all_y.shape)\n",
    "print('X shape: ', all_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "test_loss, test_acc = model.evaluate(all_X, all_y)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "predictions = model.predict(all_X)\n",
    "rounded_predictions = [int(float(round(x[0]))) for x in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC AUC\n",
    "print('ROC AUC: ', roc_auc_score(all_y, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Chems :', sider_mod.ChemicalID.nunique())\n",
    "print('Dis :', sider_mod.MESH.nunique())\n",
    "print('chem:dis obs: ', sider_mod.shape[0])\n",
    "print('of which are uncorrelated: ', sider_mod[sider_mod.Correlation == 0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error out to stop notebook\n",
    "for a in i\n",
    "def \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Examine the predictions\n",
    "Let's look at the predictions the NN gets wrong, see if there's a pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create df with the relevant data\n",
    "test_set = df1.loc[test_rows]\n",
    "test_set['Predictions'] = predictions\n",
    "test_set['RoundPredictions'] = rounded_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimise memory --> set col types for the incoming CSV\n",
    "cds_cols = ['# ChemicalName', 'ChemicalID', 'DiseaseName', 'DiseaseID', 'DirectEvidence']\n",
    "cd_col_types = {   \n",
    "    '# ChemicalName': 'category',\n",
    "    'ChemicalID': 'category',\n",
    "    'DiseaseName': 'category',\n",
    "    'DiseaseID': 'category',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the disease and chemical names back! For the sake of eyeballing for patterns\n",
    "# Read in CTD csv, skipping the intro rows\n",
    "df_cd = pd.read_csv('../ctd-to-nt/csvs/CTD_chemicals_diseases.csv', usecols=cds_cols, dtype=cd_col_types, skiprows=27)\n",
    "df_cd = df_cd.drop(0)\n",
    "df_cd = df_cd.dropna(subset=['DirectEvidence']) # drop if it doesn't have direct evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set.DiseaseID.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['Correlation'] = test_set.Correlation.astype(np.uint8)\n",
    "print(mem_usage(test_set['RoundPredictions']))\n",
    "test_set['RoundPredictions'] = test_set.RoundPredictions.astype(np.uint8)\n",
    "print(mem_usage(test_set['RoundPredictions']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [chem_dis, df, df_d, df_c, Dvecs, Cvecs]\n",
    "del lst\n",
    "test_set = test_set.drop(['DVec', 'CVec', 'index'], axis=1) # memory intensive\n",
    "for col in ['DiseaseID', 'ChemicalID', 'DiseaseName', '# ChemicalName', 'DirectEvidence']:\n",
    "    print(col,  df_cd.columns)\n",
    "    if str(col) in df_cd.columns: print('sd') # df_cd[col] = df_cd[col].astype('category')\n",
    "    if col in test_set.columns: test_set[col] = test_set[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_usage(df_cd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge in the names\n",
    "\n",
    "# Because this weirdly requires a tonne of memory, let's optimise (for stupid terrible top-of-range dell laptop)\n",
    "# lst = [chem_dis, df, df_d, df_c, Dvecs, Cvecs]\n",
    "# del lst\n",
    "# test_set = test_set.drop(['DVec', 'CVec', 'index'], axis=1) # memory intensive\n",
    "# for col in ['DiseaseID', 'ChemicalID', 'DiseaseName', '# ChemicalName', 'DirectEvidence']:\n",
    "#     if col in df_cd.columns: df_cd[col] = df_cd[col].astype('category')\n",
    "#     if col in test_set.columns: test_set[col] = test_set[col].astype('category')\n",
    "\n",
    "test_set = pd.merge(test_set, df_cd[['DiseaseID', 'DiseaseName']], on='DiseaseID')\n",
    "test_set = pd.merge(test_set, df_cd[['# ChemicalName', 'ChemicalID']], on='ChemicalID')\n",
    "\n",
    "# weirdly these operations introduce millions of duplicate rows, so delete duplicates:\n",
    "test_set = test_set.drop_duplicates(list(set(test_set.columns.values))) #- set(['DVec','CVec'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cd.ChemicalID = df_cd.ChemicalID.astype('category')\n",
    "type(df_cd.ChemicalID[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set[['DiseaseName', '# ChemicalName', 'Correlation', 'Predictions', 'RoundPredictions']].to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import gofunction counts (for each disease and each chem). This csv was output in opa2vec.ipynb\n",
    "gofunc_counts = pd.read_csv('gofunc_counts.csv')\n",
    "test_set = pd.merge(test_set, gofunc_counts[['ChemicalID', 'gofunc']], on='ChemicalID')\n",
    "test_set = test_set.rename(columns = {'gofunc':'ChemGoFuncs'})\n",
    "test_set = pd.merge(test_set, gofunc_counts[['DiseaseID', 'gofunc']], on='DiseaseID')\n",
    "test_set = test_set.rename(columns = {'gofunc':'DisGoFuncs'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This cell is pointless - manually verifying accuracy test\n",
    "# # Round predictions to int based on threshold, run accuracy-test manually\n",
    "# predictions = model.predict(X_test)\n",
    "# threshold = predictions[:].sum()/len(predictions) # Threshold is the mean value of predictions\n",
    "# predictions = [float(round(x[0]-threshold+0.5)) for x in predictions]\n",
    "# manual_accuracy = sklearn.metrics.accuracy_score(y_test, predictions, normalize=True, sample_weight=None)\n",
    "# print(manual_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Calculate Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate out the cosine similarity and see if there's a difference between groups\n",
    "# def cosine_sim (row):\n",
    "#     return cosine_similarity(np.array(row.DVec).reshape(1, -1), np.array(row.CVec).reshape(1, -1))[0][0]\n",
    "\n",
    "# df1['cosine_sim'] = df1.apply(lambda row: cosine_sim(row), axis=1)\n",
    "\n",
    "# # Compare cosine sim of correlated and uncorrelated groups\n",
    "# print('Cosine mean with no correlation: ', df1[df1.Correlation == 1 ].cosine_sim.mean())\n",
    "# print('Cosine mean with correlation: ', df1[df1.Correlation == 0 ].cosine_sim.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save entire model to a HDF5 file\n",
    "model.save('nn020219auc907.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model (in two files, one for weights and one for json)\n",
    "json_string = model.to_json()\n",
    "model.save_weights(\"WEIGHTSnn020219auc907.h5\")\n",
    "with open('nn020219auc907.json', 'w') as outfile:\n",
    "    json.dump(json_string, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model.save('nn020219auc907.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "del model  # deletes the existing model\n",
    "\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "# model = load_model('nn020219auc907.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.default_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.saved_model import tag_constants\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    with tf.Session as sess:\n",
    "        ...\n",
    "\n",
    "        # Saving\n",
    "        inputs = {\n",
    "            \"batch_size_placeholder\": batch_size_placeholder,\n",
    "            \"features_placeholder\": features_placeholder,\n",
    "            \"labels_placeholder\": labels_placeholder,\n",
    "        }\n",
    "        outputs = {\"prediction\": model_output}\n",
    "        tf.saved_model.simple_save(\n",
    "            sess, './', inputs, outputs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = tf.Variable(tf.truncated_normal(shape=[10]), name='w1')\n",
    "w2 = tf.Variable(tf.truncated_normal(shape=[20]), name='w2')\n",
    "tf.add_to_collection('vars', w1)\n",
    "tf.add_to_collection('vars', w2)\n",
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver.save(sess, 'my-model')\n",
    "# `save` method will call `export_meta_graph` implicitly.\n",
    "# you will get saved graph files:my-model.meta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

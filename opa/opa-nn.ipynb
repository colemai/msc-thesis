{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook: Use NN to predict disease from chemicals using Opa2Vec vectors\n",
    "<b> Author: </b> Ian Coleman <br>\n",
    "<b> Purpose: </b> Take the vectors created in the opa2vec notebook. This took chemical go functions\n",
    "    and disease go function, creating vectors for each. Train a NN to predict positive chem-dis relationships from these vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO\n",
    "# Hyperparameter tuning:\n",
    "# 1. How many uncorrelated pairs do we want\n",
    "# 2. Epochs\n",
    "# 3. Batch size\n",
    "# 4. Number of layers, number of nodes per layer\n",
    "# 5. Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from pandas_ml import ConfusionMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Vectors and Pre-Process them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import vec file\n",
    "with open('AllVectorResults.lst', 'r') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip and split vector data into list of lists [chem, vec]\n",
    "text = text.replace('\\n', '')\n",
    "text = text.split(']')\n",
    "text = [item.strip().split(' [') for item in text]\n",
    "\n",
    "# Turn it into a data frame\n",
    "df = pd.DataFrame(text)\n",
    "df.columns = ['ID', 'Vector']\n",
    "\n",
    "# Clean\n",
    "df = df.dropna()\n",
    "df['Vector'] = df.Vector.map(lambda x: x.rstrip().lstrip().replace('    ', ' ').replace('   ', ' ').replace('  ', ' ').replace(' ', ','))\n",
    "\n",
    "# Turn vector column into a list\n",
    "df['Vector'] = df.Vector.map(lambda x: x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MESH:D012559</td>\n",
       "      <td>[0.01491615, -0.00155747, -0.30986652, 0.04035...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MESH:D009404</td>\n",
       "      <td>[3.82804424e-02, 1.29408345e-01, 3.75053808e-0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MESH:D001749</td>\n",
       "      <td>[-0.01025235, 0.00664143, -0.30367315, 0.15593...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MESH:D011471</td>\n",
       "      <td>[-0.0130785, -0.02445601, -0.46697775, 0.13181...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MESH:D008106</td>\n",
       "      <td>[-0.06240484, 0.00166245, -0.5013923, 0.116841...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID                                             Vector\n",
       "0  MESH:D012559  [0.01491615, -0.00155747, -0.30986652, 0.04035...\n",
       "1  MESH:D009404  [3.82804424e-02, 1.29408345e-01, 3.75053808e-0...\n",
       "2  MESH:D001749  [-0.01025235, 0.00664143, -0.30367315, 0.15593...\n",
       "3  MESH:D011471  [-0.0130785, -0.02445601, -0.46697775, 0.13181...\n",
       "4  MESH:D008106  [-0.06240484, 0.00166245, -0.5013923, 0.116841..."
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create DF for NN\n",
    "Munge the df into the following columns:<br>\n",
    "ChemID DisID ChemVec DisVec PositiveAssociationExists(binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ChemicalID</th>\n",
       "      <th>DiseaseID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C112297</td>\n",
       "      <td>MESH:D006948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C112297</td>\n",
       "      <td>MESH:D012640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C425777</td>\n",
       "      <td>MESH:D006948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C013567</td>\n",
       "      <td>MESH:D006333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C418863</td>\n",
       "      <td>MESH:D013262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ChemicalID     DiseaseID\n",
       "0    C112297  MESH:D006948\n",
       "1    C112297  MESH:D012640\n",
       "2    C425777  MESH:D006948\n",
       "3    C013567  MESH:D006333\n",
       "4    C418863  MESH:D013262"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Import file of proven chem-dis positive associations (created in ctd-to-nt notebook from ctd data)\n",
    "chem_dis = pd.read_csv('../ctd-to-nt/chem-=assocs.csv')\n",
    "chem_dis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of any chems/diseases that don't have a vector\n",
    "chem_dis['DiseaseID'] = chem_dis['DiseaseID'].astype(str)\n",
    "df['ID'] = df['ID'].astype(str)\n",
    "id_list = df.ID.tolist() # list of chems+diseases with vecs\n",
    "\n",
    "chem_dis['hasDVec'] = chem_dis.DiseaseID.map(lambda x: x in id_list)\n",
    "chem_dis['hasCVec'] = chem_dis.ChemicalID.map(lambda x: x in id_list)\n",
    "chem_dis = chem_dis.loc[(chem_dis['hasDVec'] == True) & (chem_dis['hasCVec'] == True)]\n",
    "chem_dis = chem_dis.drop(['hasDVec','hasCVec'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all info into one df\n",
    "# this df now contains only correlated diseases and vecs\n",
    "df_d = df.copy()\n",
    "df_d.columns= ['DiseaseID', 'DVec']\n",
    "df_c = df.copy()\n",
    "df_c.columns= ['ChemicalID', 'CVec']\n",
    "df1 = pd.merge(chem_dis, df_d, on='DiseaseID')\n",
    "df1 = pd.merge(df1, df_c, on='ChemicalID')\n",
    "\n",
    "df1['Correlation'] = 1 # currently only have correlated in there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate dfs of dis-vecs and chem-vecs ( in order to generate additional rows for df1)\n",
    "dis = df.ID.map(lambda x: ('MESH' in x) | ('OMIM' in x))\n",
    "chems = df.ID.map(lambda x: ('MESH' not in x) & ('OMIM' not in x))\n",
    "\n",
    "df_chems = df[chems]\n",
    "df_dis = df[dis]\n",
    "df_chems = df_chems.reset_index(drop=True)\n",
    "df_dis = df_dis.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  13318\n",
      "(19978, 5)\n",
      "(19681, 5)\n"
     ]
    }
   ],
   "source": [
    "# Add unrelated pairs to df1\n",
    "no_rows = (df1.shape[0]-1) * 2 # This is a parameter to be tuned --> how many uncorrelated pairs do we want\n",
    "print('shape: ', no_rows)\n",
    "\n",
    "# Randomly select chems and diseases (as many as there are related pairs)\n",
    "np.random.seed(1606)\n",
    "no_chems = len(df_chems) -1\n",
    "no_dis = len(df_dis) -1\n",
    "rand_chems = np.random.choice(no_chems, no_rows, replace=True)\n",
    "rand_dis = np.random.choice(no_dis, no_rows, replace=True)\n",
    "\n",
    "# Add the new pairs as rows\n",
    "for x in range(0, no_rows):\n",
    "    int1 = rand_chems[x]\n",
    "    int2 = rand_dis[x]\n",
    "    chem, chemvec = df_chems.loc[int1, 'ID'], df_chems.loc[int1, 'Vector']\n",
    "    dis, disvec = df_dis.loc[int2, 'ID'], df_dis.loc[int2, 'Vector']\n",
    "    df1 = df1.append({'ChemicalID':chem, 'DiseaseID':dis, 'CVec':chemvec, 'DVec':disvec, 'Correlation':0}, ignore_index=True)\n",
    "\n",
    "print(df1.shape)\n",
    "# Drop any duplicates (removes known correlated pairs accidentally generated as uncorrelated)\n",
    "df1 = df1.drop_duplicates(subset=['ChemicalID', 'DiseaseID'], keep=False)\n",
    "print(df1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the elements of the vectors to actual numbers\n",
    "df1['DVec'] = df1.DVec.map(lambda x: [float(i) for i in x])\n",
    "df1['CVec'] = df1.CVec.map(lambda x: [float(i) for i in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocess\n",
    "Now that we have the df ready, let's split it into train/test/validation sets and convert it into numpy arrays so it can be consumed by a Keras NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ChemicalID</th>\n",
       "      <th>DiseaseID</th>\n",
       "      <th>DVec</th>\n",
       "      <th>CVec</th>\n",
       "      <th>Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C049584</td>\n",
       "      <td>MESH:D001943</td>\n",
       "      <td>[-0.00754089653, 0.0284954235, -0.145941272, -...</td>\n",
       "      <td>[0.02189679, 0.10079688, 0.04159389, -0.099326...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C049584</td>\n",
       "      <td>MESH:D018270</td>\n",
       "      <td>[0.01976116, 0.098279193, 0.0369541571, -0.089...</td>\n",
       "      <td>[0.02189679, 0.10079688, 0.04159389, -0.099326...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C049584</td>\n",
       "      <td>MESH:D019457</td>\n",
       "      <td>[0.03360923, 0.10056757, 0.05314376, -0.113913...</td>\n",
       "      <td>[0.02189679, 0.10079688, 0.04159389, -0.099326...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C049584</td>\n",
       "      <td>MESH:D003110</td>\n",
       "      <td>[0.00136586, 0.13832065, 0.02338981, -0.113038...</td>\n",
       "      <td>[0.02189679, 0.10079688, 0.04159389, -0.099326...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C049584</td>\n",
       "      <td>MESH:D015179</td>\n",
       "      <td>[-0.02237691, 0.07948194, -0.09784327, -0.0321...</td>\n",
       "      <td>[0.02189679, 0.10079688, 0.04159389, -0.099326...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ChemicalID     DiseaseID                                               DVec  \\\n",
       "0    C049584  MESH:D001943  [-0.00754089653, 0.0284954235, -0.145941272, -...   \n",
       "1    C049584  MESH:D018270  [0.01976116, 0.098279193, 0.0369541571, -0.089...   \n",
       "2    C049584  MESH:D019457  [0.03360923, 0.10056757, 0.05314376, -0.113913...   \n",
       "3    C049584  MESH:D003110  [0.00136586, 0.13832065, 0.02338981, -0.113038...   \n",
       "4    C049584  MESH:D015179  [-0.02237691, 0.07948194, -0.09784327, -0.0321...   \n",
       "\n",
       "                                                CVec  Correlation  \n",
       "0  [0.02189679, 0.10079688, 0.04159389, -0.099326...            1  \n",
       "1  [0.02189679, 0.10079688, 0.04159389, -0.099326...            1  \n",
       "2  [0.02189679, 0.10079688, 0.04159389, -0.099326...            1  \n",
       "3  [0.02189679, 0.10079688, 0.04159389, -0.099326...            1  \n",
       "4  [0.02189679, 0.10079688, 0.04159389, -0.099326...            1  "
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Keras, need to turn inputs into numpy arrays instead of pandas df\n",
    "# First create single np array of the two vectors CONCERN: should these be two separate inputs?\n",
    "Dvecs = pd.DataFrame(df1.DVec.values.tolist(), index= df1.index)\n",
    "Cvecs = pd.DataFrame(df1.CVec.values.tolist(), index= df1.index)\n",
    "all_X = Dvecs.merge(Cvecs, how='outer', left_index=True, right_index=True)\n",
    "all_X = np.array(all_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create np array of the y output\n",
    "all_y = np.array(df1.Correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape:  (19681,)\n",
      "X shape:  (19681, 400)\n"
     ]
    }
   ],
   "source": [
    "print('y shape: ', all_y.shape)\n",
    "print('X shape: ', all_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train, test, val\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_X, all_y, test_size=0.2, random_state=1606)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1606)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Establish NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Establish the model architecture\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(200, activation=tf.nn.relu), #Dense layers are fully connected, this one 128 nodes\n",
    "    keras.layers.Dropout(rate=0.2, noise_shape=None, seed=None),\n",
    "    keras.layers.Dense(200, activation=tf.nn.relu), #it's safe to say that I don't know what I'm doing here\n",
    "    keras.layers.Dropout(rate=0.2, noise_shape=None, seed=None),\n",
    "    keras.layers.Dense(10, activation=tf.nn.relu),\n",
    "    keras.layers.Dropout(rate=0.2, noise_shape=None, seed=None),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])\n",
    "# ??? How is the number of nodes decided? Final layer has no. of outcome vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Compile the model (give it loss func, optimise func and eval metric)\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(), # determines how the model is adapted based on loss func\n",
    "              loss='binary_crossentropy', # measure of accuracy during training\n",
    "              metrics=['accuracy']) # measure for train and testing steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-training, set up training params\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=8, verbose=0, mode='auto', baseline=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10076 samples, validate on 2519 samples\n",
      "Epoch 1/150\n",
      "10076/10076 [==============================] - 4s 356us/step - loss: 0.5246 - acc: 0.7567 - val_loss: 0.5087 - val_acc: 0.7697\n",
      "Epoch 2/150\n",
      "10076/10076 [==============================] - 3s 326us/step - loss: 0.4902 - acc: 0.7753 - val_loss: 0.5018 - val_acc: 0.7626\n",
      "Epoch 3/150\n",
      "10076/10076 [==============================] - 3s 323us/step - loss: 0.4811 - acc: 0.7813 - val_loss: 0.5480 - val_acc: 0.7499\n",
      "Epoch 4/150\n",
      "10076/10076 [==============================] - 3s 307us/step - loss: 0.4726 - acc: 0.7857 - val_loss: 0.4719 - val_acc: 0.7813\n",
      "Epoch 5/150\n",
      "10076/10076 [==============================] - 5s 461us/step - loss: 0.4596 - acc: 0.7915 - val_loss: 0.4596 - val_acc: 0.7940\n",
      "Epoch 6/150\n",
      "10076/10076 [==============================] - 5s 473us/step - loss: 0.4532 - acc: 0.7956 - val_loss: 0.4563 - val_acc: 0.7979\n",
      "Epoch 7/150\n",
      "10076/10076 [==============================] - 3s 306us/step - loss: 0.4461 - acc: 0.7985 - val_loss: 0.4510 - val_acc: 0.7995\n",
      "Epoch 8/150\n",
      "10076/10076 [==============================] - 3s 296us/step - loss: 0.4380 - acc: 0.8045 - val_loss: 0.4480 - val_acc: 0.7960\n",
      "Epoch 9/150\n",
      "10076/10076 [==============================] - 3s 291us/step - loss: 0.4335 - acc: 0.8024 - val_loss: 0.4427 - val_acc: 0.8063\n",
      "Epoch 10/150\n",
      "10076/10076 [==============================] - 3s 298us/step - loss: 0.4337 - acc: 0.8037 - val_loss: 0.4295 - val_acc: 0.8043\n",
      "Epoch 11/150\n",
      "10076/10076 [==============================] - 3s 303us/step - loss: 0.4255 - acc: 0.8079 - val_loss: 0.4292 - val_acc: 0.8055\n",
      "Epoch 12/150\n",
      "10076/10076 [==============================] - 3s 302us/step - loss: 0.4169 - acc: 0.8149 - val_loss: 0.4211 - val_acc: 0.8087\n",
      "Epoch 13/150\n",
      "10076/10076 [==============================] - 3s 323us/step - loss: 0.4221 - acc: 0.8075 - val_loss: 0.4484 - val_acc: 0.7892\n",
      "Epoch 14/150\n",
      "10076/10076 [==============================] - 3s 335us/step - loss: 0.4163 - acc: 0.8136 - val_loss: 0.4413 - val_acc: 0.8043\n",
      "Epoch 15/150\n",
      "10076/10076 [==============================] - 3s 318us/step - loss: 0.4095 - acc: 0.8164 - val_loss: 0.4322 - val_acc: 0.8110\n",
      "Epoch 16/150\n",
      "10076/10076 [==============================] - 4s 355us/step - loss: 0.4062 - acc: 0.8181 - val_loss: 0.4432 - val_acc: 0.7960\n",
      "Epoch 17/150\n",
      "10076/10076 [==============================] - 4s 425us/step - loss: 0.4037 - acc: 0.8217 - val_loss: 0.4173 - val_acc: 0.8138\n",
      "Epoch 18/150\n",
      "10076/10076 [==============================] - 4s 441us/step - loss: 0.3959 - acc: 0.8204 - val_loss: 0.4263 - val_acc: 0.8023\n",
      "Epoch 19/150\n",
      "10076/10076 [==============================] - 4s 436us/step - loss: 0.3962 - acc: 0.8221 - val_loss: 0.4103 - val_acc: 0.8142\n",
      "Epoch 20/150\n",
      "10076/10076 [==============================] - 5s 458us/step - loss: 0.3920 - acc: 0.8247 - val_loss: 0.4172 - val_acc: 0.8079\n",
      "Epoch 21/150\n",
      "10076/10076 [==============================] - 5s 451us/step - loss: 0.3872 - acc: 0.8248 - val_loss: 0.4260 - val_acc: 0.8122\n",
      "Epoch 22/150\n",
      "10076/10076 [==============================] - 4s 433us/step - loss: 0.3844 - acc: 0.8308 - val_loss: 0.4091 - val_acc: 0.8237\n",
      "Epoch 23/150\n",
      "10076/10076 [==============================] - 5s 474us/step - loss: 0.3808 - acc: 0.8285 - val_loss: 0.4145 - val_acc: 0.8098\n",
      "Epoch 24/150\n",
      "10076/10076 [==============================] - 5s 455us/step - loss: 0.3843 - acc: 0.8258 - val_loss: 0.4023 - val_acc: 0.8182\n",
      "Epoch 25/150\n",
      "10076/10076 [==============================] - 5s 453us/step - loss: 0.3690 - acc: 0.8395 - val_loss: 0.4195 - val_acc: 0.8079\n",
      "Epoch 26/150\n",
      "10076/10076 [==============================] - 4s 446us/step - loss: 0.3748 - acc: 0.8325 - val_loss: 0.3888 - val_acc: 0.8293\n",
      "Epoch 27/150\n",
      "10076/10076 [==============================] - 4s 438us/step - loss: 0.3688 - acc: 0.8380 - val_loss: 0.4077 - val_acc: 0.8186\n",
      "Epoch 28/150\n",
      "10076/10076 [==============================] - 4s 436us/step - loss: 0.3733 - acc: 0.8302 - val_loss: 0.3929 - val_acc: 0.8313\n",
      "Epoch 29/150\n",
      "10076/10076 [==============================] - 4s 440us/step - loss: 0.3666 - acc: 0.8350 - val_loss: 0.4345 - val_acc: 0.7983\n",
      "Epoch 30/150\n",
      "10076/10076 [==============================] - 4s 437us/step - loss: 0.3750 - acc: 0.8317 - val_loss: 0.4195 - val_acc: 0.8043\n",
      "Epoch 31/150\n",
      "10076/10076 [==============================] - 4s 443us/step - loss: 0.3567 - acc: 0.8429 - val_loss: 0.4446 - val_acc: 0.7856\n",
      "Epoch 32/150\n",
      "10076/10076 [==============================] - 5s 464us/step - loss: 0.3587 - acc: 0.8396 - val_loss: 0.4124 - val_acc: 0.8075\n",
      "Epoch 33/150\n",
      "10076/10076 [==============================] - 4s 435us/step - loss: 0.3602 - acc: 0.8373 - val_loss: 0.3862 - val_acc: 0.8206\n",
      "Epoch 34/150\n",
      "10076/10076 [==============================] - 5s 447us/step - loss: 0.3518 - acc: 0.8450 - val_loss: 0.3846 - val_acc: 0.8198\n",
      "Epoch 35/150\n",
      "10076/10076 [==============================] - 5s 465us/step - loss: 0.3530 - acc: 0.8405 - val_loss: 0.3850 - val_acc: 0.8273\n",
      "Epoch 36/150\n",
      "10076/10076 [==============================] - 3s 340us/step - loss: 0.3505 - acc: 0.8452 - val_loss: 0.4230 - val_acc: 0.8122\n",
      "Epoch 37/150\n",
      "10076/10076 [==============================] - 3s 305us/step - loss: 0.3433 - acc: 0.8464 - val_loss: 0.4078 - val_acc: 0.8166\n",
      "Epoch 38/150\n",
      "10076/10076 [==============================] - 3s 316us/step - loss: 0.3451 - acc: 0.8463 - val_loss: 0.3936 - val_acc: 0.8257\n",
      "Epoch 39/150\n",
      "10076/10076 [==============================] - 3s 306us/step - loss: 0.3394 - acc: 0.8477 - val_loss: 0.3808 - val_acc: 0.8345\n",
      "Epoch 40/150\n",
      "10076/10076 [==============================] - 3s 317us/step - loss: 0.3401 - acc: 0.8486 - val_loss: 0.4215 - val_acc: 0.8154\n",
      "Epoch 41/150\n",
      "10076/10076 [==============================] - 3s 306us/step - loss: 0.3406 - acc: 0.8482 - val_loss: 0.3748 - val_acc: 0.8261\n",
      "Epoch 42/150\n",
      "10076/10076 [==============================] - 3s 311us/step - loss: 0.3387 - acc: 0.8499 - val_loss: 0.3779 - val_acc: 0.8281\n",
      "Epoch 43/150\n",
      "10076/10076 [==============================] - 3s 315us/step - loss: 0.3390 - acc: 0.8496 - val_loss: 0.3952 - val_acc: 0.8225\n",
      "Epoch 44/150\n",
      "10076/10076 [==============================] - 3s 311us/step - loss: 0.3324 - acc: 0.8529 - val_loss: 0.4278 - val_acc: 0.8126\n",
      "Epoch 45/150\n",
      "10076/10076 [==============================] - 3s 308us/step - loss: 0.3249 - acc: 0.8534 - val_loss: 0.3649 - val_acc: 0.8384\n",
      "Epoch 46/150\n",
      "10076/10076 [==============================] - 3s 311us/step - loss: 0.3274 - acc: 0.8568 - val_loss: 0.4256 - val_acc: 0.8114\n",
      "Epoch 47/150\n",
      "10076/10076 [==============================] - 3s 308us/step - loss: 0.3236 - acc: 0.8558 - val_loss: 0.3915 - val_acc: 0.8229\n",
      "Epoch 48/150\n",
      "10076/10076 [==============================] - 3s 308us/step - loss: 0.3173 - acc: 0.8599 - val_loss: 0.3812 - val_acc: 0.8273\n",
      "Epoch 49/150\n",
      "10076/10076 [==============================] - 3s 317us/step - loss: 0.3185 - acc: 0.8573 - val_loss: 0.3787 - val_acc: 0.8237\n",
      "Epoch 50/150\n",
      "10076/10076 [==============================] - 3s 313us/step - loss: 0.3190 - acc: 0.8562 - val_loss: 0.3782 - val_acc: 0.8285\n",
      "Epoch 51/150\n",
      "10076/10076 [==============================] - 3s 307us/step - loss: 0.3162 - acc: 0.8626 - val_loss: 0.3661 - val_acc: 0.8305\n",
      "Epoch 52/150\n",
      "10076/10076 [==============================] - 3s 305us/step - loss: 0.3097 - acc: 0.8631 - val_loss: 0.3697 - val_acc: 0.8353\n",
      "Epoch 53/150\n",
      "10076/10076 [==============================] - 3s 299us/step - loss: 0.3110 - acc: 0.8623 - val_loss: 0.4102 - val_acc: 0.8142\n",
      "Epoch 54/150\n",
      "10076/10076 [==============================] - 3s 297us/step - loss: 0.3141 - acc: 0.8592 - val_loss: 0.3939 - val_acc: 0.8214\n",
      "Epoch 55/150\n",
      "10076/10076 [==============================] - 3s 297us/step - loss: 0.3032 - acc: 0.8676 - val_loss: 0.4434 - val_acc: 0.8222\n",
      "Epoch 56/150\n",
      "10076/10076 [==============================] - 3s 295us/step - loss: 0.3097 - acc: 0.8655 - val_loss: 0.3608 - val_acc: 0.8444\n",
      "Epoch 57/150\n",
      "10076/10076 [==============================] - 3s 309us/step - loss: 0.3076 - acc: 0.8651 - val_loss: 0.3635 - val_acc: 0.8341\n",
      "Epoch 58/150\n",
      "10076/10076 [==============================] - 3s 296us/step - loss: 0.2988 - acc: 0.8679 - val_loss: 0.3691 - val_acc: 0.8321\n",
      "Epoch 59/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10076/10076 [==============================] - 3s 283us/step - loss: 0.3002 - acc: 0.8706 - val_loss: 0.3715 - val_acc: 0.8416\n",
      "Epoch 60/150\n",
      "10076/10076 [==============================] - 3s 292us/step - loss: 0.3018 - acc: 0.8664 - val_loss: 0.3947 - val_acc: 0.8305\n",
      "Epoch 61/150\n",
      "10076/10076 [==============================] - 3s 287us/step - loss: 0.2948 - acc: 0.8717 - val_loss: 0.3622 - val_acc: 0.8341\n",
      "Epoch 62/150\n",
      "10076/10076 [==============================] - 3s 284us/step - loss: 0.2951 - acc: 0.8711 - val_loss: 0.3887 - val_acc: 0.8317\n",
      "Epoch 63/150\n",
      "10076/10076 [==============================] - 3s 285us/step - loss: 0.2962 - acc: 0.8695 - val_loss: 0.3771 - val_acc: 0.8392\n",
      "Epoch 64/150\n",
      "10076/10076 [==============================] - 3s 290us/step - loss: 0.2829 - acc: 0.8784 - val_loss: 0.3665 - val_acc: 0.8396\n",
      "Epoch 65/150\n",
      "10076/10076 [==============================] - 3s 299us/step - loss: 0.2929 - acc: 0.8710 - val_loss: 0.3643 - val_acc: 0.8356\n",
      "Epoch 66/150\n",
      "10076/10076 [==============================] - 3s 282us/step - loss: 0.2877 - acc: 0.8755 - val_loss: 0.3694 - val_acc: 0.8285\n",
      "Epoch 67/150\n",
      "10076/10076 [==============================] - 3s 289us/step - loss: 0.2852 - acc: 0.8760 - val_loss: 0.3574 - val_acc: 0.8392\n",
      "Epoch 68/150\n",
      "10076/10076 [==============================] - 3s 287us/step - loss: 0.2875 - acc: 0.8732 - val_loss: 0.3753 - val_acc: 0.8281\n",
      "Epoch 69/150\n",
      "10076/10076 [==============================] - 3s 291us/step - loss: 0.2932 - acc: 0.8741 - val_loss: 0.4040 - val_acc: 0.8225\n",
      "Epoch 70/150\n",
      "10076/10076 [==============================] - 3s 286us/step - loss: 0.2810 - acc: 0.8749 - val_loss: 0.3814 - val_acc: 0.8313\n",
      "Epoch 71/150\n",
      "10076/10076 [==============================] - 3s 301us/step - loss: 0.2838 - acc: 0.8718 - val_loss: 0.3630 - val_acc: 0.8404\n",
      "Epoch 72/150\n",
      "10076/10076 [==============================] - 3s 292us/step - loss: 0.2814 - acc: 0.8774 - val_loss: 0.3615 - val_acc: 0.8321\n",
      "Epoch 73/150\n",
      "10076/10076 [==============================] - 3s 299us/step - loss: 0.2842 - acc: 0.8742 - val_loss: 0.4813 - val_acc: 0.8098\n",
      "Epoch 74/150\n",
      "10076/10076 [==============================] - 3s 298us/step - loss: 0.2709 - acc: 0.8847 - val_loss: 0.3730 - val_acc: 0.8305\n",
      "Epoch 75/150\n",
      "10076/10076 [==============================] - 3s 288us/step - loss: 0.2747 - acc: 0.8790 - val_loss: 0.3704 - val_acc: 0.8380\n",
      "Epoch 76/150\n",
      "10076/10076 [==============================] - 3s 297us/step - loss: 0.2724 - acc: 0.8829 - val_loss: 0.3491 - val_acc: 0.8460\n",
      "Epoch 77/150\n",
      "10076/10076 [==============================] - 3s 295us/step - loss: 0.2671 - acc: 0.8852 - val_loss: 0.3683 - val_acc: 0.8293\n",
      "Epoch 78/150\n",
      "10076/10076 [==============================] - 3s 303us/step - loss: 0.2764 - acc: 0.8829 - val_loss: 0.3452 - val_acc: 0.8511\n",
      "Epoch 79/150\n",
      "10076/10076 [==============================] - 3s 293us/step - loss: 0.2677 - acc: 0.8828 - val_loss: 0.3580 - val_acc: 0.8440\n",
      "Epoch 80/150\n",
      "10076/10076 [==============================] - 3s 291us/step - loss: 0.2671 - acc: 0.8853 - val_loss: 0.3669 - val_acc: 0.8412\n",
      "Epoch 81/150\n",
      "10076/10076 [==============================] - 3s 298us/step - loss: 0.2715 - acc: 0.8832 - val_loss: 0.3908 - val_acc: 0.8257\n",
      "Epoch 82/150\n",
      "10076/10076 [==============================] - 3s 294us/step - loss: 0.2691 - acc: 0.8815 - val_loss: 0.3485 - val_acc: 0.8464\n",
      "Epoch 83/150\n",
      "10076/10076 [==============================] - 3s 299us/step - loss: 0.2585 - acc: 0.8868 - val_loss: 0.3822 - val_acc: 0.8289\n",
      "Epoch 84/150\n",
      "10076/10076 [==============================] - 3s 300us/step - loss: 0.2673 - acc: 0.8821 - val_loss: 0.3410 - val_acc: 0.8515\n",
      "Epoch 85/150\n",
      "10076/10076 [==============================] - 3s 309us/step - loss: 0.2643 - acc: 0.8852 - val_loss: 0.3566 - val_acc: 0.8472\n",
      "Epoch 86/150\n",
      "10076/10076 [==============================] - 3s 299us/step - loss: 0.2563 - acc: 0.8883 - val_loss: 0.3504 - val_acc: 0.8456\n",
      "Epoch 87/150\n",
      "10076/10076 [==============================] - 3s 299us/step - loss: 0.2533 - acc: 0.8914 - val_loss: 0.3728 - val_acc: 0.8337\n",
      "Epoch 88/150\n",
      "10076/10076 [==============================] - 3s 297us/step - loss: 0.2578 - acc: 0.8891 - val_loss: 0.3512 - val_acc: 0.8436\n",
      "Epoch 89/150\n",
      "10076/10076 [==============================] - 3s 309us/step - loss: 0.2566 - acc: 0.8902 - val_loss: 0.3546 - val_acc: 0.8507\n",
      "Epoch 90/150\n",
      "10076/10076 [==============================] - 3s 299us/step - loss: 0.2523 - acc: 0.8902 - val_loss: 0.3435 - val_acc: 0.8503\n",
      "Epoch 91/150\n",
      "10076/10076 [==============================] - 3s 316us/step - loss: 0.2541 - acc: 0.8882 - val_loss: 0.3833 - val_acc: 0.8317\n",
      "Epoch 92/150\n",
      "10076/10076 [==============================] - 3s 301us/step - loss: 0.2514 - acc: 0.8934 - val_loss: 0.3633 - val_acc: 0.8400\n",
      "Epoch 93/150\n",
      "10076/10076 [==============================] - 3s 316us/step - loss: 0.2471 - acc: 0.8946 - val_loss: 0.3506 - val_acc: 0.8440\n",
      "Epoch 94/150\n",
      "10076/10076 [==============================] - 3s 305us/step - loss: 0.2544 - acc: 0.8893 - val_loss: 0.3933 - val_acc: 0.8261\n",
      "Epoch 95/150\n",
      "10076/10076 [==============================] - 3s 304us/step - loss: 0.2492 - acc: 0.8902 - val_loss: 0.3406 - val_acc: 0.8535\n",
      "Epoch 96/150\n",
      "10076/10076 [==============================] - 3s 306us/step - loss: 0.2484 - acc: 0.8923 - val_loss: 0.3768 - val_acc: 0.8360\n",
      "Epoch 97/150\n",
      "10076/10076 [==============================] - 3s 304us/step - loss: 0.2417 - acc: 0.8938 - val_loss: 0.3411 - val_acc: 0.8452\n",
      "Epoch 98/150\n",
      "10076/10076 [==============================] - 3s 324us/step - loss: 0.2404 - acc: 0.8933 - val_loss: 0.3752 - val_acc: 0.8444\n",
      "Epoch 99/150\n",
      "10076/10076 [==============================] - 3s 315us/step - loss: 0.2473 - acc: 0.8940 - val_loss: 0.3570 - val_acc: 0.8452\n",
      "Epoch 100/150\n",
      "10076/10076 [==============================] - 3s 300us/step - loss: 0.2380 - acc: 0.8966 - val_loss: 0.4001 - val_acc: 0.8293\n",
      "Epoch 101/150\n",
      "10076/10076 [==============================] - 3s 311us/step - loss: 0.2374 - acc: 0.8973 - val_loss: 0.3627 - val_acc: 0.8424\n",
      "Epoch 102/150\n",
      "10076/10076 [==============================] - 3s 315us/step - loss: 0.2377 - acc: 0.8985 - val_loss: 0.4031 - val_acc: 0.8190\n",
      "Epoch 103/150\n",
      "10076/10076 [==============================] - 4s 377us/step - loss: 0.2432 - acc: 0.8915 - val_loss: 0.3563 - val_acc: 0.8452\n",
      "Epoch 104/150\n",
      "10076/10076 [==============================] - 4s 435us/step - loss: 0.2362 - acc: 0.8966 - val_loss: 0.3899 - val_acc: 0.8321\n",
      "Epoch 105/150\n",
      "10076/10076 [==============================] - 3s 317us/step - loss: 0.2298 - acc: 0.8993 - val_loss: 0.3706 - val_acc: 0.8432\n",
      "Epoch 106/150\n",
      "10076/10076 [==============================] - 4s 382us/step - loss: 0.2370 - acc: 0.8996 - val_loss: 0.3567 - val_acc: 0.8448\n",
      "Epoch 107/150\n",
      "10076/10076 [==============================] - 4s 394us/step - loss: 0.2254 - acc: 0.9031 - val_loss: 0.3583 - val_acc: 0.8487\n",
      "Epoch 108/150\n",
      "10076/10076 [==============================] - 2s 246us/step - loss: 0.2344 - acc: 0.8988 - val_loss: 0.3510 - val_acc: 0.8448\n",
      "Epoch 109/150\n",
      "10076/10076 [==============================] - 1s 130us/step - loss: 0.2284 - acc: 0.9035 - val_loss: 0.3457 - val_acc: 0.8519\n",
      "Epoch 110/150\n",
      "10076/10076 [==============================] - 2s 217us/step - loss: 0.2346 - acc: 0.9007 - val_loss: 0.3452 - val_acc: 0.8472\n",
      "Epoch 111/150\n",
      "10076/10076 [==============================] - 1s 141us/step - loss: 0.2230 - acc: 0.9030 - val_loss: 0.3903 - val_acc: 0.8376\n",
      "Epoch 112/150\n",
      "10076/10076 [==============================] - 3s 327us/step - loss: 0.2222 - acc: 0.9047 - val_loss: 0.3593 - val_acc: 0.8567\n",
      "Epoch 113/150\n",
      "10076/10076 [==============================] - 3s 305us/step - loss: 0.2211 - acc: 0.9043 - val_loss: 0.4492 - val_acc: 0.8269\n",
      "Epoch 114/150\n",
      "10076/10076 [==============================] - 1s 143us/step - loss: 0.2216 - acc: 0.9056 - val_loss: 0.3648 - val_acc: 0.8396\n",
      "Epoch 115/150\n",
      "10076/10076 [==============================] - 1s 125us/step - loss: 0.2161 - acc: 0.9069 - val_loss: 0.3826 - val_acc: 0.8341\n",
      "Epoch 116/150\n",
      "10076/10076 [==============================] - 1s 132us/step - loss: 0.2160 - acc: 0.9097 - val_loss: 0.3811 - val_acc: 0.8476\n",
      "Epoch 117/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10076/10076 [==============================] - 1s 138us/step - loss: 0.2196 - acc: 0.9063 - val_loss: 0.5255 - val_acc: 0.8051\n",
      "Epoch 118/150\n",
      "10076/10076 [==============================] - 1s 126us/step - loss: 0.2097 - acc: 0.9117 - val_loss: 0.3901 - val_acc: 0.8432\n",
      "Epoch 119/150\n",
      "10076/10076 [==============================] - 1s 126us/step - loss: 0.2175 - acc: 0.9068 - val_loss: 0.3957 - val_acc: 0.8345\n",
      "Epoch 120/150\n",
      "10076/10076 [==============================] - 1s 123us/step - loss: 0.2223 - acc: 0.9038 - val_loss: 0.3591 - val_acc: 0.8507\n",
      "Epoch 121/150\n",
      "10076/10076 [==============================] - 1s 125us/step - loss: 0.2149 - acc: 0.9093 - val_loss: 0.3887 - val_acc: 0.8345\n",
      "Epoch 122/150\n",
      "10076/10076 [==============================] - 1s 123us/step - loss: 0.2195 - acc: 0.9063 - val_loss: 0.4405 - val_acc: 0.8305\n",
      "Epoch 123/150\n",
      "10076/10076 [==============================] - 1s 123us/step - loss: 0.2084 - acc: 0.9121 - val_loss: 0.3897 - val_acc: 0.8468\n",
      "Epoch 124/150\n",
      "10076/10076 [==============================] - 1s 123us/step - loss: 0.2074 - acc: 0.9125 - val_loss: 0.3549 - val_acc: 0.8452\n",
      "Epoch 125/150\n",
      "10076/10076 [==============================] - 1s 124us/step - loss: 0.1999 - acc: 0.9142 - val_loss: 0.4644 - val_acc: 0.8257\n",
      "Epoch 126/150\n",
      "10076/10076 [==============================] - 1s 126us/step - loss: 0.2101 - acc: 0.9119 - val_loss: 0.3726 - val_acc: 0.8519\n",
      "Epoch 127/150\n",
      "10076/10076 [==============================] - 1s 138us/step - loss: 0.2138 - acc: 0.9088 - val_loss: 0.3767 - val_acc: 0.8476\n",
      "Epoch 128/150\n",
      "10076/10076 [==============================] - 1s 126us/step - loss: 0.1983 - acc: 0.9163 - val_loss: 0.3721 - val_acc: 0.8448\n",
      "Epoch 129/150\n",
      "10076/10076 [==============================] - 1s 123us/step - loss: 0.2072 - acc: 0.9120 - val_loss: 0.3697 - val_acc: 0.8535\n",
      "Epoch 130/150\n",
      "10076/10076 [==============================] - 1s 127us/step - loss: 0.1983 - acc: 0.9145 - val_loss: 0.3988 - val_acc: 0.8476\n",
      "Epoch 131/150\n",
      "10076/10076 [==============================] - 1s 126us/step - loss: 0.2132 - acc: 0.9061 - val_loss: 0.3693 - val_acc: 0.8523\n",
      "Epoch 132/150\n",
      "10076/10076 [==============================] - 1s 127us/step - loss: 0.1996 - acc: 0.9157 - val_loss: 0.3724 - val_acc: 0.8547\n",
      "Epoch 133/150\n",
      "10076/10076 [==============================] - 1s 126us/step - loss: 0.1931 - acc: 0.9164 - val_loss: 0.5133 - val_acc: 0.8154\n",
      "Epoch 134/150\n",
      "10076/10076 [==============================] - 1s 125us/step - loss: 0.1990 - acc: 0.9164 - val_loss: 0.3767 - val_acc: 0.8448\n",
      "Epoch 135/150\n",
      "10076/10076 [==============================] - 1s 125us/step - loss: 0.1962 - acc: 0.9159 - val_loss: 0.3994 - val_acc: 0.8456\n",
      "Epoch 136/150\n",
      "10076/10076 [==============================] - 1s 124us/step - loss: 0.1968 - acc: 0.9172 - val_loss: 0.3818 - val_acc: 0.8559\n",
      "Epoch 137/150\n",
      "10076/10076 [==============================] - 1s 123us/step - loss: 0.1878 - acc: 0.9182 - val_loss: 0.4052 - val_acc: 0.8408\n",
      "Epoch 138/150\n",
      "10076/10076 [==============================] - 1s 125us/step - loss: 0.1990 - acc: 0.9162 - val_loss: 0.3720 - val_acc: 0.8583\n",
      "Epoch 139/150\n",
      "10076/10076 [==============================] - 1s 124us/step - loss: 0.1882 - acc: 0.9214 - val_loss: 0.3840 - val_acc: 0.8539\n",
      "Epoch 140/150\n",
      "10076/10076 [==============================] - 1s 125us/step - loss: 0.1957 - acc: 0.9160 - val_loss: 0.3861 - val_acc: 0.8440\n",
      "Epoch 141/150\n",
      "10076/10076 [==============================] - 1s 125us/step - loss: 0.1971 - acc: 0.9133 - val_loss: 0.4080 - val_acc: 0.8444\n",
      "Epoch 142/150\n",
      "10076/10076 [==============================] - 1s 125us/step - loss: 0.1896 - acc: 0.9198 - val_loss: 0.4563 - val_acc: 0.8249\n",
      "Epoch 143/150\n",
      "10076/10076 [==============================] - 1s 123us/step - loss: 0.1848 - acc: 0.9209 - val_loss: 0.4123 - val_acc: 0.8575\n",
      "Epoch 144/150\n",
      "10076/10076 [==============================] - 1s 126us/step - loss: 0.1920 - acc: 0.9166 - val_loss: 0.3810 - val_acc: 0.8595\n",
      "Epoch 145/150\n",
      "10076/10076 [==============================] - 1s 123us/step - loss: 0.1796 - acc: 0.9229 - val_loss: 0.3747 - val_acc: 0.8464\n",
      "Epoch 146/150\n",
      "10076/10076 [==============================] - 1s 124us/step - loss: 0.1857 - acc: 0.9231 - val_loss: 0.3909 - val_acc: 0.8491\n",
      "Epoch 147/150\n",
      "10076/10076 [==============================] - 1s 141us/step - loss: 0.1823 - acc: 0.9259 - val_loss: 0.3930 - val_acc: 0.8412\n",
      "Epoch 148/150\n",
      "10076/10076 [==============================] - 1s 122us/step - loss: 0.1796 - acc: 0.9222 - val_loss: 0.4032 - val_acc: 0.8440\n",
      "Epoch 149/150\n",
      "10076/10076 [==============================] - 1s 126us/step - loss: 0.1769 - acc: 0.9257 - val_loss: 0.3964 - val_acc: 0.8519\n",
      "Epoch 150/150\n",
      "10076/10076 [==============================] - 1s 123us/step - loss: 0.1789 - acc: 0.9231 - val_loss: 0.3843 - val_acc: 0.8626\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0a41fc01d0>"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Train\n",
    "model.fit(X_train, y_train, validation_split=0.2, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3937/3937 [==============================] - 0s 60us/step\n",
      "Test accuracy: 0.8646177292354584\n"
     ]
    }
   ],
   "source": [
    "# 4. Evaluate\n",
    "# Accuracy\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get actual predictions for test set\n",
    "predictions = model.predict(X_test)\n",
    "rounded_predictions = [int(float(round(x[0]))) for x in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "Predicted  False  True  __all__\n",
      "Actual                         \n",
      "False       2302   282     2584\n",
      "True         251  1102     1353\n",
      "__all__     2553  1384     3937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f0a2f8645c0>"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# Confusion Matrix\n",
    "\n",
    "confusion_matrix = ConfusionMatrix(y_test, rounded_predictions)\n",
    "print(\"Confusion matrix:\\n%s\" % confusion_matrix)\n",
    "confusion_matrix.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC:  0.9352815609847627\n"
     ]
    }
   ],
   "source": [
    "# ROC AUC\n",
    "print('ROC AUC: ', roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This is pointless - manually verifying accuracy test\n",
    "# # Round predictions to int based on threshold, run accuracy-test manually\n",
    "# predictions = model.predict(X_test)\n",
    "# threshold = predictions[:].sum()/len(predictions) # Threshold is the mean value of predictions\n",
    "# predictions = [float(round(x[0]-threshold+0.5)) for x in predictions]\n",
    "# manual_accuracy = sklearn.metrics.accuracy_score(y_test, predictions, normalize=True, sample_weight=None)\n",
    "# print(manual_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calculate Cosine Similary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate out the cosine similarity and see if there's a difference between groups\n",
    "# def cosine_sim (row):\n",
    "#     return cosine_similarity(np.array(row.DVec).reshape(1, -1), np.array(row.CVec).reshape(1, -1))[0][0]\n",
    "\n",
    "# df1['cosine_sim'] = df1.apply(lambda row: cosine_sim(row), axis=1)\n",
    "\n",
    "# # Compare cosine sim of correlated and uncorrelated groups\n",
    "# print('Cosine mean with no correlation: ', df1[df1.Correlation == 1 ].cosine_sim.mean())\n",
    "# print('Cosine mean with correlation: ', df1[df1.Correlation == 0 ].cosine_sim.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model (in two files, one for weights and one for json)\n",
    "json_string = model.to_json()\n",
    "model.save_weights(\"model1-0.94.h5\")\n",
    "with open('model1-0.94.json', 'w') as outfile:\n",
    "    json.dump(json_string, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'AllVectorResults.lst\\nassociations_c.txt\\nassociations_d.txt\\nCTD_genes.csv.gz.1\\nctd-to-vec.ipynb\\ndata\\ndeleteme.lst\\ndiseases.lst\\nentities.lst\\nfinalclasses.lst\\ngeneIDs.txt\\nmodel.h5\\nmyassociations\\nopa2vec.ipynb\\nopa-nn-expand-phenotype.ipynb\\nopa-nn.ipynb\\nuniprotIDs.txt\\n'"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.json', 'w') as outfile:\n",
    "    json.dump(json.string, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

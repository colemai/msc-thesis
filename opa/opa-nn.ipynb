{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook: Use NN to predict disease from chemicals using Opa2Vec vectors\n",
    "<b> Author: </b> Ian Coleman <br>\n",
    "<b> Purpose: </b> Take the vectors created in the opa2vec notebook. This took chemical go functions\n",
    "    and disease go function, creating vectors for each. Train a NN to predict positive chem-dis relationships from these vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from pandas_ml import ConfusionMatrix\n",
    "import json\n",
    "import subprocess\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "#Set random seed\n",
    "np.random.seed(1606)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mem_usage(pandas_obj):\n",
    "    if isinstance(pandas_obj,pd.DataFrame):\n",
    "        usage_b = pandas_obj.memory_usage(deep=True).sum()\n",
    "    else: # we assume if not a df it's a series\n",
    "        usage_b = pandas_obj.memory_usage(deep=True)\n",
    "    usage_mb = usage_b / 1024 ** 2 # convert bytes to megabytes\n",
    "    return \"{:03.2f} MB\".format(usage_mb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Vectors and Pre-Process them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Gofunc vec file\n",
    "with open('go-gofuncs.lst', 'r') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip and split vector data into list of lists [chem, vec]\n",
    "text = text.replace('\\n', '')\n",
    "text = text.split(']')\n",
    "text = [item.strip().split(' [') for item in text]\n",
    "\n",
    "# Turn it into a data frame\n",
    "df = pd.DataFrame(text)\n",
    "df.columns = ['ID', 'Vector']\n",
    "\n",
    "# Clean\n",
    "df = df.dropna()\n",
    "df['Vector'] = df.Vector.map(lambda x: x.rstrip().lstrip().replace('    ', ' ').replace('   ', ' ').replace('  ', ' ').replace(' ', ','))\n",
    "\n",
    "# Turn vector column into a list\n",
    "df['Vector'] = df.Vector.map(lambda x: x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MESH:D012559</td>\n",
       "      <td>[1.76247600e-02, -1.05403718e-02, -4.61959302e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MESH:D009404</td>\n",
       "      <td>[0.01795662, 0.13640046, 0.03051887, -0.100085...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MESH:D001749</td>\n",
       "      <td>[-8.68742093e-02, 8.83234814e-02, -2.54237115e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MESH:D011471</td>\n",
       "      <td>[-0.00926186, 0.04098112, -0.4911138, -0.22025...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MESH:D008106</td>\n",
       "      <td>[-0.12722802, 0.07976454, -0.5775048, -0.28237...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID                                             Vector\n",
       "0  MESH:D012559  [1.76247600e-02, -1.05403718e-02, -4.61959302e...\n",
       "1  MESH:D009404  [0.01795662, 0.13640046, 0.03051887, -0.100085...\n",
       "2  MESH:D001749  [-8.68742093e-02, 8.83234814e-02, -2.54237115e...\n",
       "3  MESH:D011471  [-0.00926186, 0.04098112, -0.4911138, -0.22025...\n",
       "4  MESH:D008106  [-0.12722802, 0.07976454, -0.5775048, -0.28237..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create DF for NN\n",
    "Munge the df into the following columns:<br>\n",
    "ChemID DisID ChemVec DisVec PositiveAssociationExists(binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ChemicalID</th>\n",
       "      <th>DiseaseID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C112297</td>\n",
       "      <td>MESH:D006948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C112297</td>\n",
       "      <td>MESH:D012640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C425777</td>\n",
       "      <td>MESH:D006948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C013567</td>\n",
       "      <td>MESH:D006333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C418863</td>\n",
       "      <td>MESH:D013262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ChemicalID     DiseaseID\n",
       "0    C112297  MESH:D006948\n",
       "1    C112297  MESH:D012640\n",
       "2    C425777  MESH:D006948\n",
       "3    C013567  MESH:D006333\n",
       "4    C418863  MESH:D013262"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Import file of proven chem-dis positive associations (created in ctd-to-nt notebook from ctd data)\n",
    "chem_dis = pd.read_csv('../ctd-to-nt/chem-dis-pos-assocs.csv')\n",
    "chem_dis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of any chems/diseases that don't have a vector\n",
    "chem_dis['DiseaseID'] = chem_dis['DiseaseID'].astype(str)\n",
    "df['ID'] = df['ID'].astype(str)\n",
    "id_list = df.ID.tolist() # list of chems+diseases with vecs\n",
    "\n",
    "chem_dis['hasDVec'] = chem_dis.DiseaseID.map(lambda x: x in id_list)\n",
    "chem_dis['hasCVec'] = chem_dis.ChemicalID.map(lambda x: x in id_list)\n",
    "chem_dis = chem_dis.loc[(chem_dis['hasDVec'] == True) & (chem_dis['hasCVec'] == True)]\n",
    "chem_dis = chem_dis.drop(['hasDVec','hasCVec'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all info into one df\n",
    "# this df now contains only correlated diseases and vecs\n",
    "df_d = df.copy()\n",
    "df_d.columns= ['DiseaseID', 'DVec']\n",
    "df_c = df.copy()\n",
    "df_c.columns= ['ChemicalID', 'CVec']\n",
    "df1 = pd.merge(chem_dis, df_d, on='DiseaseID')\n",
    "df1 = pd.merge(df1, df_c, on='ChemicalID')\n",
    "\n",
    "df1['Correlation'] = 1 # currently only have correlated in there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ChemicalID</th>\n",
       "      <th>DiseaseID</th>\n",
       "      <th>DVec</th>\n",
       "      <th>CVec</th>\n",
       "      <th>Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C028474</td>\n",
       "      <td>MESH:D006528</td>\n",
       "      <td>[-0.08689959, 0.06080057, -0.04620415, -0.1237...</td>\n",
       "      <td>[0.01018825, 0.08274926, 0.05085841, -0.117527...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C028474</td>\n",
       "      <td>MESH:D005355</td>\n",
       "      <td>[-4.32693306e-03, 1.35906458e-01, -1.91942360e...</td>\n",
       "      <td>[0.01018825, 0.08274926, 0.05085841, -0.117527...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C028474</td>\n",
       "      <td>MESH:D006529</td>\n",
       "      <td>[-0.02542116, 0.0981225, -0.01938446, -0.14929...</td>\n",
       "      <td>[0.01018825, 0.08274926, 0.05085841, -0.117527...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C028474</td>\n",
       "      <td>MESH:D006965</td>\n",
       "      <td>[-0.01135238, 0.143319, 0.04601676, -0.1474806...</td>\n",
       "      <td>[0.01018825, 0.08274926, 0.05085841, -0.117527...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C028474</td>\n",
       "      <td>MESH:D008114</td>\n",
       "      <td>[-0.10265561, 0.03210206, -0.13152453, -0.0728...</td>\n",
       "      <td>[0.01018825, 0.08274926, 0.05085841, -0.117527...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ChemicalID     DiseaseID                                               DVec  \\\n",
       "0    C028474  MESH:D006528  [-0.08689959, 0.06080057, -0.04620415, -0.1237...   \n",
       "1    C028474  MESH:D005355  [-4.32693306e-03, 1.35906458e-01, -1.91942360e...   \n",
       "2    C028474  MESH:D006529  [-0.02542116, 0.0981225, -0.01938446, -0.14929...   \n",
       "3    C028474  MESH:D006965  [-0.01135238, 0.143319, 0.04601676, -0.1474806...   \n",
       "4    C028474  MESH:D008114  [-0.10265561, 0.03210206, -0.13152453, -0.0728...   \n",
       "\n",
       "                                                CVec  Correlation  \n",
       "0  [0.01018825, 0.08274926, 0.05085841, -0.117527...            1  \n",
       "1  [0.01018825, 0.08274926, 0.05085841, -0.117527...            1  \n",
       "2  [0.01018825, 0.08274926, 0.05085841, -0.117527...            1  \n",
       "3  [0.01018825, 0.08274926, 0.05085841, -0.117527...            1  \n",
       "4  [0.01018825, 0.08274926, 0.05085841, -0.117527...            1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3137"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8651, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chem_dis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate dfs of dis-vecs and chem-vecs ( in order to generate additional rows for df1)\n",
    "dis = df.ID.map(lambda x: ('MESH' in x) | ('OMIM' in x))\n",
    "chems = df.ID.map(lambda x: ('MESH' not in x) & ('OMIM' not in x))\n",
    "\n",
    "df_chems = df[chems]\n",
    "df_dis = df[dis]\n",
    "df_chems = df_chems.reset_index(drop=True)\n",
    "df_dis = df_dis.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  8650\n",
      "(17301, 5)\n",
      "(17145, 5)\n"
     ]
    }
   ],
   "source": [
    "# Add unrelated pairs to df1\n",
    "no_rows = (df1.shape[0]-1)   # This is a parameter to be tuned --> how many uncorrelated pairs do we want\n",
    "print('shape: ', no_rows)\n",
    "\n",
    "# Randomly select chems and diseases (as many as there are related pairs)\n",
    "no_chems = len(df_chems) -1\n",
    "no_dis = len(df_dis) -1\n",
    "rand_chems = np.random.choice(no_chems, no_rows, replace=True)\n",
    "rand_dis = np.random.choice(no_dis, no_rows, replace=True)\n",
    "\n",
    "# Add the new pairs as rows\n",
    "for x in range(0, no_rows):\n",
    "    int1 = rand_chems[x]\n",
    "    int2 = rand_dis[x]\n",
    "    chem, chemvec = df_chems.loc[int1, 'ID'], df_chems.loc[int1, 'Vector']\n",
    "    dis, disvec = df_dis.loc[int2, 'ID'], df_dis.loc[int2, 'Vector']\n",
    "    df1 = df1.append({'ChemicalID':chem, 'DiseaseID':dis, 'CVec':chemvec, 'DVec':disvec, 'Correlation':0}, ignore_index=True)\n",
    "\n",
    "print(df1.shape)\n",
    "# Drop any duplicates (removes known correlated pairs accidentally generated as uncorrelated)\n",
    "df1 = df1.drop_duplicates(subset=['ChemicalID', 'DiseaseID'], keep=False)\n",
    "print(df1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the elements of the vectors to actual numbers\n",
    "df1['DVec'] = df1.DVec.map(lambda x: [float(i) for i in x])\n",
    "df1['CVec'] = df1.CVec.map(lambda x: [float(i) for i in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Phenotype Vecs\n",
    "Got a list of Chem-Phenotypes from Sara Alth, where did these come from originally?\n",
    "They have CID identifiers (Pubchem). Need to convert CTD ID to CID ID\n",
    "Use API like so \n",
    "http://pubchem.ncbi.nlm.nih.gov/rest/pug/substance/sourceid/Comparative%20Toxicogenomics%20Database/C533207/cids/TXT/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## First we'll add DOIDs for diseases and CIDs for chemicals as an intermediate for adding phenotypes\n",
    "# # Read in CSV mapping chems to CID and dis to DOID --> This file is created by phens-opa2vec.ipynb\n",
    "# mapper = pd.read_csv('entities.lst')\n",
    "\n",
    "# # Make the maps from this\n",
    "# dis_map = dict(zip(mapper.ID, mapper.DOID))\n",
    "# chem_map = dict(zip(mapper.ID, mapper.CID))\n",
    "\n",
    "# # Apply the maps to df1\n",
    "# df1['DOID'] = df1.DiseaseID.map(lambda x: dis_map.get(x))\n",
    "# df1['CID'] = df1.ChemicalID.map(lambda x: chem_map.get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read in the association files from Sara\n",
    "# dis_phens = pd.read_csv('Disease-PhenotypeAssocation.txt', sep=' ', names=['DOID', 'Phenotype'])\n",
    "# chem_phens =  pd.read_csv('Drug-PhenotypeAssocation.txt', sep=' ', names=['CID', 'Phenotype'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The association files have a different format for each ID system than mine, homogenise these\n",
    "# # first chem\n",
    "# def cid_standardiser (cid):\n",
    "#     # Must be format CID + 9 int chars, starting with 1 seemingly\n",
    "#     cid = int(cid)\n",
    "#     output = 'CID1' + '0' * (8 - len(str(cid))) + str(cid)\n",
    "#     return output\n",
    "\n",
    "# df1['CID'] = df1.CID.map(lambda x: np.nan if math.isnan(x) else cid_standardiser(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # and now disease \n",
    "# def doid_standardiser (doid):\n",
    "#     # Must be format DOID_ + ...\n",
    "#     # I'm unsure about how well this is working so print out the DOIDs that don't match\n",
    "#     doid = doid.replace(':', '_')\n",
    "# #     output = 'CID1' + '0' * (8 - len(str(cid))) + str(cid)\n",
    "#     return doid\n",
    "\n",
    "# df1['DOID'] = df1.DOID.map(lambda x: np.nan if isinstance(x, float) else doid_standardiser(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now that the DOIDs and CIDs are theoretically standardised we can chuck in the phens to df1\n",
    "\n",
    "# First though check which DOIDs and CIDs do not match up --> SEEMS OK\n",
    "# test_doids = df1[df1.DOID.map(lambda x: isinstance(x, str))].DOID.tolist()\n",
    "# imported_doids = dis_phens.DOID.tolist()\n",
    "# for item in test_doids:\n",
    "#     if item not in imported_doids:\n",
    "#         print(item)                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's create one associations file for each ontology (I believe we need to run HP and MP separately)\n",
    "# chem_phens.columns = ['ID', 'Phen']\n",
    "# dis_phens.columns = ['ID', 'Phen']\n",
    "\n",
    "# # Maps for Split into MP/HP\n",
    "# hp_df_crit_d = dis_phens.Phen.map(lambda x: 'obolibrary.org/obo/HP' in str(x))\n",
    "# mp_df_crit_d = dis_phens.Phen.map(lambda x: 'obolibrary.org/obo/MP' in str(x))\n",
    "# hp_df_crit_c = chem_phens.Phen.map(lambda x: 'obolibrary.org/obo/HP' in str(x))\n",
    "# mp_df_crit_c = chem_phens.Phen.map(lambda x: 'obolibrary.org/obo/MP' in str(x))\n",
    "\n",
    "# total_hp = chem_phens[hp_df_crit_c].append(dis_phens[hp_df_crit_d], ignore_index=True)\n",
    "# total_mp = chem_phens[mp_df_crit_c].append(dis_phens[mp_df_crit_d], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export the association files and print some stats (more stats later)\n",
    "# np.savetxt(r'associations_hp.txt', total_hp.values, fmt='%s')\n",
    "# np.savetxt(r'associations_mp.txt', total_mp.values, fmt='%s')\n",
    "\n",
    "# print('Num HP associations: ', total_hp.shape[0])\n",
    "# print('Num MP associations: ', total_mp.shape[0])\n",
    "# print('Num ents with MP phen assocs: ', len(total_mp.ID.unique()))\n",
    "# print('Num ents with HP phen assocs: ', len(total_hp.ID.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create entities.lst to inform opa2vec which entities we want vectors for\n",
    "# entities = total_mp.ID.unique().tolist()\n",
    "# np.savetxt(r'entities_mp.lst', entities, fmt='%s')\n",
    "\n",
    "# entities = total_hp.ID.unique().tolist()\n",
    "# np.savetxt(r'entities_hp.lst', entities, fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run OPA2VEC for entity-phens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HP --> takes ages so commenting out\n",
    "# subprocess.check_output('(cd ../../opa2vec/ ; python2 runOPA2Vec.py -ontology ../ontologies/hp.owl -associations ../msc-thesis/opa/associations_hp.txt -entities ../msc-thesis/opa/entities_hp.lst -outfile ../msc-thesis/opa/hpvecs.lst)', shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MP --> takes ages so commenting out\n",
    "# subprocess.check_output('(cd ../../opa2vec/ ; python2 runOPA2Vec.py -ontology ../ontologies/mp.owl -associations ../msc-thesis/opa/associations_mp.txt -entities ../msc-thesis/opa/entities_mp.lst -outfile ../msc-thesis/opa/mpvecs.lst)', shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Now import and integrate the vecs\n",
    "# # Import vec file\n",
    "# with open('hpvecs.lst', 'r') as file:\n",
    "#     text = file.read()\n",
    "    \n",
    "# # Strip and split vector data into list of lists [chem, vec]\n",
    "# text = text.replace('\\n', '')\n",
    "# text = text.split(']')\n",
    "# text = [item.strip().split(' [') for item in text]\n",
    "\n",
    "# # Turn it into a data frame\n",
    "# hp_vecs = pd.DataFrame(text)\n",
    "# hp_vecs.columns = ['ID', 'Vector']\n",
    "\n",
    "# # Clean\n",
    "# hp_vecs = hp_vecs.dropna()\n",
    "# hp_vecs['Vector'] = hp_vecs.Vector.map(lambda x: x.rstrip().lstrip().replace('    ', ' ').replace('   ', ' ').replace('  ', ' ').replace(' ', ','))\n",
    "\n",
    "# # Turn vector column into a list\n",
    "# hp_vecs['Vector'] = hp_vecs.Vector.map(lambda x: x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Now import and integrate the mp vecs\n",
    "# # Import vec file\n",
    "# with open('mpvecs.lst', 'r') as file:\n",
    "#     text = file.read()\n",
    "\n",
    "# # Strip and split vector data into list of lists [chem, vec]\n",
    "# text = text.replace('\\n', '')\n",
    "# text = text.split(']')\n",
    "# text = [item.strip().split(' [') for item in text]\n",
    "\n",
    "# # Turn it into a data frame\n",
    "# mp_vecs = pd.DataFrame(text)\n",
    "# mp_vecs.columns = ['ID', 'Vector']\n",
    "\n",
    "# # Clean\n",
    "# mp_vecs = mp_vecs.dropna()\n",
    "# mp_vecs['Vector'] = mp_vecs.Vector.map(lambda x: x.rstrip().lstrip().replace('    ', ' ').replace('   ', ' ').replace('  ', ' ').replace(' ', ','))\n",
    "\n",
    "# # Turn vector column into a list\n",
    "# mp_vecs['Vector'] = mp_vecs.Vector.map(lambda x: x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right, now I have hp and mp vecs, match them up into df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make Maps:\n",
    "# mp_ent_to_vec = dict(zip(mp_vecs.ID, mp_vecs.Vector))\n",
    "# hp_ent_to_vec = dict(zip(hp_vecs.ID, hp_vecs.Vector))\n",
    "\n",
    "# # Map entities to vecs (one set of vecs for mp and another for hp)\n",
    "# df1['disPhenVecMP'] = df1.DOID.map(lambda x: mp_ent_to_vec.get(x))\n",
    "# df1['disPhenVecHP'] = df1.DOID.map(lambda x: hp_ent_to_vec.get(x))\n",
    "\n",
    "# df1['chemPhenVecHP'] = df1.CID.map(lambda x: hp_ent_to_vec.get(x))\n",
    "# df1['chemPhenVecMP'] = df1.CID.map(lambda x: mp_ent_to_vec.get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## TODO note that I have removed rows without gofuncVecs, maybe now they should be kept\n",
    "# # Print Stats\n",
    "# print('Note these numbers include both correlated and uncorrelated pairs')\n",
    "# print('Number of rows with gofuncs: ', df1.shape[0]) ##NB change this if keeping rows w/o gofunc vecs\n",
    "# print('Number of rows with dis mp vec: ', df1[df1.disPhenVecMP.map(lambda x: x is not None)].shape[0])\n",
    "# print('Number of rows with dis hp vec: ', df1[df1.disPhenVecHP.map(lambda x: x is not None)].shape[0])\n",
    "# print('Number of rows with chem mp vec: ', df1[df1.chemPhenVecMP.map(lambda x: x is not None)].shape[0])\n",
    "# print('Number of rows with chem hp vec: ', df1[df1.chemPhenVecHP.map(lambda x: x is not None)].shape[0])\n",
    "# no_dis_phen_vecs = df1.disPhenVecHP.map(lambda x: x is None) & df1.disPhenVecMP.map(lambda x: x is None)\n",
    "# no_chem_phen_vecs = df1.chemPhenVecHP.map(lambda x: x is None) & df1.chemPhenVecMP.map(lambda x: x is None)\n",
    "# no_phen_vecs = no_dis_phen_vecs & no_chem_phen_vecs\n",
    "# print('Number of rows with no phen vecs: ', df1[no_phen_vecs].shape[0])\n",
    "# all_dis_phen_vecs = df1.disPhenVecHP.map(lambda x: x is not None) & df1.disPhenVecMP.map(lambda x: x is not None)\n",
    "# all_chem_phen_vecs = df1.chemPhenVecHP.map(lambda x: x is not None) & df1.chemPhenVecMP.map(lambda x: x is not None)\n",
    "# all_vecs = all_dis_phen_vecs & all_chem_phen_vecs\n",
    "# all_vecs_pos_corr = all_vecs & df1.Correlation.map(lambda x: x == 1)\n",
    "# all_vecs_neg_corr = all_vecs & df1.Correlation.map(lambda x: x == 0)\n",
    "\n",
    "# print('Number of rows with everything', df1[all_vecs].shape[0])\n",
    "\n",
    "# print('Number of correlated pairs with everything', df1[all_vecs_pos_corr].shape[0])\n",
    "# print('Number of uncorrelated pairs with everything', df1[all_vecs_neg_corr].shape[0])\n",
    "\n",
    "# print('total pos corr', df1[df1.Correlation.map(lambda x: x == 1)].shape[0])\n",
    "# print('total neg corr', df1[df1.Correlation.map(lambda x: x == 0)].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add empty vecs for rows that don't have phen vecs\n",
    "# empty_vec = [0] * 200\n",
    "\n",
    "# for col in ['disPhenVecMP', 'disPhenVecHP', 'chemPhenVecHP', 'chemPhenVecMP']:\n",
    "#     df1[col] = df1[col].map(lambda x: empty_vec if x is None else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Change the Phen vec elements from string to floats\n",
    "# df1['disPhenVecHP'] = df1.disPhenVecHP.map(lambda x: [float(i) for i in x])\n",
    "# df1['disPhenVecMP'] = df1.disPhenVecMP.map(lambda x: [float(i) for i in x])\n",
    "# df1['chemPhenVecHP'] = df1.chemPhenVecHP.map(lambda x: [float(i) for i in x])\n",
    "# df1['chemPhenVecMP'] = df1.chemPhenVecMP.map(lambda x: [float(i) for i in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add CHEBI Vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First add CHEBI IDs for each chem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chem_names = df1.ChemicalID.unique().tolist()\n",
    "# np.savetxt(r'chem_names', total_hp.values, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To add chebi first need CID for ALL Chems so I made this comprehensive map:\n",
    "# Load the map from pickle object\n",
    "def load_obj(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "ctd_cid_map = load_obj('ctd_cid_map')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['CID'] = df1.ChemicalID.map(lambda x: ctd_cid_map.get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ChemicalID</th>\n",
       "      <th>DiseaseID</th>\n",
       "      <th>DVec</th>\n",
       "      <th>CVec</th>\n",
       "      <th>Correlation</th>\n",
       "      <th>CID</th>\n",
       "      <th>CHEBI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4921</th>\n",
       "      <td>D005557</td>\n",
       "      <td>MESH:D007153</td>\n",
       "      <td>[0.00656568538, 0.140204757, 0.044911094, -0.1...</td>\n",
       "      <td>[0.0154880257, 0.106162332, 0.0615323484, -0.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CHEBI:16842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17093</th>\n",
       "      <td>D001554</td>\n",
       "      <td>MESH:C567543</td>\n",
       "      <td>[0.0198122, 0.10710014, 0.04138577, -0.1024861...</td>\n",
       "      <td>[0.0147514781, 0.0977057293, 0.0615985803, -0....</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CHEBI:16716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14350</th>\n",
       "      <td>D000433</td>\n",
       "      <td>MESH:D013163</td>\n",
       "      <td>[-0.01963329, 0.06567155, -0.20128179, -0.1522...</td>\n",
       "      <td>[0.04787027, 0.10445248, 0.03111667, -0.128154...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CHEBI:28831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ChemicalID     DiseaseID  \\\n",
       "4921     D005557  MESH:D007153   \n",
       "17093    D001554  MESH:C567543   \n",
       "14350    D000433  MESH:D013163   \n",
       "\n",
       "                                                    DVec  \\\n",
       "4921   [0.00656568538, 0.140204757, 0.044911094, -0.1...   \n",
       "17093  [0.0198122, 0.10710014, 0.04138577, -0.1024861...   \n",
       "14350  [-0.01963329, 0.06567155, -0.20128179, -0.1522...   \n",
       "\n",
       "                                                    CVec  Correlation  CID  \\\n",
       "4921   [0.0154880257, 0.106162332, 0.0615323484, -0.0...            1  NaN   \n",
       "17093  [0.0147514781, 0.0977057293, 0.0615985803, -0....            0  NaN   \n",
       "14350  [0.04787027, 0.10445248, 0.03111667, -0.128154...            0  NaN   \n",
       "\n",
       "             CHEBI  \n",
       "4921   CHEBI:16842  \n",
       "17093  CHEBI:16716  \n",
       "14350  CHEBI:28831  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert CIDS from bytes to strings and export in order to make a map from CID to CHEBI ID\n",
    "df1['CID'] = df1.CID.str.decode(\"utf-8\")\n",
    "np.savetxt(r'allCIDs.txt', df1.CID.unique(), fmt='%s')\n",
    "\n",
    "## NOTE the next step is MANUAL you need to upload this allCIDs.txt to http://cts.fiehnlab.ucdavis.edu/batch \n",
    "# and download it as ctd_chebi.csv to the current folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CSV mapping CID to CHEBI\n",
    "ctdChebdf = pd.read_csv('ctd_chebi.csv')\n",
    "\n",
    "# Turn it into a dict\n",
    "ctd_chebi = dict(zip(ctdChebdf['PubChem CID'], ctdChebdf['ChEBI']))\n",
    "\n",
    "# Map the cids to chebis\n",
    "df1['CHEBI'] = df1.CID.map(lambda x: ctd_chebi.get(x))\n",
    "df1['CHEBI'] = df1.CHEBI.map(lambda x: None if x == 'No result' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now make CHEBI vecs using the CHEBI IDs\n",
    "# First an association file - just linking each chebi to its own chebi entity\n",
    "\n",
    "# add uri col\n",
    "df1['CHEBI_uri'] = df1.dropna(subset=['CHEBI']).CHEBI.map(lambda x: '<http://purl.obolibrary.org/obo/' + x.replace(':', '_') + '>')\n",
    "\n",
    "# export association file from df\n",
    "np.savetxt(r'CHEBIassociations.txt', df1[['ChemicalID', 'CHEBI_uri']].dropna().drop_duplicates().values, fmt='%s')\n",
    "\n",
    "# Now an entities file\n",
    "chems_for_cheb = df1[['ChemicalID', 'CHEBI_uri']].dropna().drop_duplicates().ChemicalID.tolist()\n",
    "np.savetxt(r'CHEBIentities.txt', chems_for_cheb, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run opa2vec on it \n",
    "subprocess.check_output('(cd ../../opa2vec/ ; python2 runOPA2Vec.py -ontology ../ontologies/chebi.owl -associations ../msc-thesis/opa/CHEBIassociations.txt -entities ../msc-thesis/opa/CHEBIentities.txt -outfile ../msc-thesis/opa/chebi-vecs.lst)', shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocess\n",
    "Now that we have the df ready, let's split it into train/test/validation sets and convert it into numpy arrays so it can be consumed by a Keras NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8595\n",
      "(17145, 5)\n"
     ]
    }
   ],
   "source": [
    "print(df1[df1.Correlation == 1].shape[0])\n",
    "print(df1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ChemicalID</th>\n",
       "      <th>DiseaseID</th>\n",
       "      <th>DVec</th>\n",
       "      <th>CVec</th>\n",
       "      <th>Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1341</th>\n",
       "      <td>C017947</td>\n",
       "      <td>MESH:D008175</td>\n",
       "      <td>[-0.0498087, 0.08013102, -0.21025404, -0.14221...</td>\n",
       "      <td>[-0.0531880707, 0.0433869548, -0.196800679, -0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9741</th>\n",
       "      <td>C582340</td>\n",
       "      <td>MESH:D013617</td>\n",
       "      <td>[0.05696682, 0.09369084, -0.0048688, -0.089769...</td>\n",
       "      <td>[0.03923435, 0.10083519, 0.0233496, -0.1123369...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ChemicalID     DiseaseID  \\\n",
       "1341    C017947  MESH:D008175   \n",
       "9741    C582340  MESH:D013617   \n",
       "\n",
       "                                                   DVec  \\\n",
       "1341  [-0.0498087, 0.08013102, -0.21025404, -0.14221...   \n",
       "9741  [0.05696682, 0.09369084, -0.0048688, -0.089769...   \n",
       "\n",
       "                                                   CVec  Correlation  \n",
       "1341  [-0.0531880707, 0.0433869548, -0.196800679, -0...            1  \n",
       "9741  [0.03923435, 0.10083519, 0.0233496, -0.1123369...            0  "
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Version for phen and gofunc vecs\n",
    "# # For Keras, need to turn inputs into numpy arrays instead of pandas df\n",
    "# # First create single np array of all vecs... not pretty:\n",
    "# Dvecs = pd.DataFrame(df1.DVec.values.tolist(), index= df1.index)\n",
    "# Cvecs = pd.DataFrame(df1.CVec.values.tolist(), index= df1.index)\n",
    "# gofuncs = Dvecs.merge(Cvecs, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "# DMPvecs = pd.DataFrame(df1.disPhenVecHP.values.tolist(), index= df1.index)\n",
    "# DHPvecs = pd.DataFrame(df1.disPhenVecMP.values.tolist(), index= df1.index)\n",
    "# disPvecs = DMPvecs.merge(DHPvecs, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "# CMPvecs = pd.DataFrame(df1.chemPhenVecHP.values.tolist(), index= df1.index)\n",
    "# CHPvecs = pd.DataFrame(df1.chemPhenVecMP.values.tolist(), index= df1.index)\n",
    "# chemPvecs = CMPvecs.merge(CHPvecs, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "# phenVecs = disPvecs.merge(chemPvecs, how='outer', left_index=True, right_index=True)\n",
    "# all_X = phenVecs.merge(gofuncs, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "# all_X = np.array(all_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version for gofunc vecs\n",
    "# For Keras, need to turn inputs into numpy arrays instead of pandas df\n",
    "# First create single np array of all vecs... not pretty:\n",
    "Dvecs = pd.DataFrame(df1.DVec.values.tolist(), index= df1.index)\n",
    "Cvecs = pd.DataFrame(df1.CVec.values.tolist(), index= df1.index)\n",
    "all_X = Dvecs.merge(Cvecs, how='outer', left_index=True, right_index=True)\n",
    "all_X = np.array(all_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create np array of the y output\n",
    "all_y = np.array(df1.Correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape:  (17145,)\n",
      "X shape:  (17145, 400)\n"
     ]
    }
   ],
   "source": [
    "print('y shape: ', all_y.shape)\n",
    "print('X shape: ', all_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create training, test, val set in a way that we can later look at the rows of each BY ROWS\n",
    "# total_rows = len(all_X)\n",
    "# row_numbers = list(range(0, total_rows))\n",
    "\n",
    "# training_rows = random.sample(row_numbers, int(round(total_rows * .6)))\n",
    "# row_numbers = set(row_numbers) - set(training_rows)\n",
    "\n",
    "# test_rows = random.sample(row_numbers, int(round(total_rows * .2)))\n",
    "# row_numbers = set(row_numbers) - set(test_rows)\n",
    "\n",
    "# val_rows = list(row_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of chemicals:  570\n",
      "number of dis:  2501\n",
      "342 114 114\n"
     ]
    }
   ],
   "source": [
    "# Split into train, test, val BY CHEMICAL instead of by random rows\n",
    "print('number of chemicals: ', len(df1.ChemicalID.unique()))\n",
    "print('number of dis: ', len(df1.DiseaseID.unique()))\n",
    "chems = list(df1.ChemicalID.unique())\n",
    "random.shuffle(chems)\n",
    "\n",
    "total_chems = len(chems)\n",
    "train_chems = chems[:round(total_chems * .6)]\n",
    "test_chems = chems[round(total_chems * .6):round(total_chems * .8)]\n",
    "val_chems = chems[round(total_chems * .8):]\n",
    "\n",
    "print(len(train_chems), len(test_chems), len(val_chems))\n",
    "\n",
    "# Now get the row numbers for each set of chemicals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['train'] = df1.ChemicalID.isin(train_chems)\n",
    "df1['test'] = df1.ChemicalID.isin(test_chems)\n",
    "df1['val'] = df1.ChemicalID.isin(val_chems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of chemicals:  570\n",
      "number of dis:  2501\n"
     ]
    }
   ],
   "source": [
    "# Split by CHEMICAL instead of by random rows\n",
    "print('number of chemicals: ', len(df1.ChemicalID.unique()))\n",
    "print('number of dis: ', len(df1.DiseaseID.unique()))\n",
    "chems = list(df1.ChemicalID.unique())\n",
    "df1 = df1.reset_index()\n",
    "training_rows = df1.index[df1.train == True].tolist()\n",
    "test_rows = df1.index[df1.test == True].tolist()\n",
    "val_rows = df1.index[df1.val == True].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10513 3509 3123\n"
     ]
    }
   ],
   "source": [
    "# Split into train, test, val\n",
    "X_train, X_test, X_val = all_X[training_rows], all_X[test_rows], all_X[val_rows]\n",
    "y_train, y_test, y_val = all_y[training_rows], all_y[test_rows], all_y[val_rows]\n",
    "\n",
    "print(len(training_rows), len(test_rows), len(val_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split into train, test, val --> OLD WAY\n",
    "# X_train, X_test, y_train, y_test = train_test_split(all_X, all_y, test_size=0.2, random_state=1606)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1606)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Establish NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Establish the model architecture\n",
    "#it's safe to say that I don't know what I'm doing here\n",
    "model = keras.Sequential([\n",
    "#     keras.layers.Dense(400, activation=tf.nn.relu), \n",
    "    keras.layers.Dense(200, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(10, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Compile the model (give it loss func, optimise func and eval metric)\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(), # determines how the model is adapted based on loss func\n",
    "              loss='binary_crossentropy', # measure of accuracy during training\n",
    "              metrics=['accuracy']) # measure for train and testing steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-training, set up training params\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10513 samples, validate on 3123 samples\n",
      "Epoch 1/10\n",
      "10513/10513 [==============================] - 3s 291us/step - loss: 0.5471 - acc: 0.7340 - val_loss: 0.5677 - val_acc: 0.7294\n",
      "Epoch 2/10\n",
      "10513/10513 [==============================] - 2s 210us/step - loss: 0.5234 - acc: 0.7491 - val_loss: 0.5669 - val_acc: 0.7230\n",
      "Epoch 3/10\n",
      "10513/10513 [==============================] - 3s 297us/step - loss: 0.5062 - acc: 0.7622 - val_loss: 0.5546 - val_acc: 0.7387\n",
      "Epoch 4/10\n",
      "10513/10513 [==============================] - 3s 307us/step - loss: 0.4995 - acc: 0.7649 - val_loss: 0.5494 - val_acc: 0.7426\n",
      "Epoch 5/10\n",
      "10513/10513 [==============================] - 2s 155us/step - loss: 0.4932 - acc: 0.7665 - val_loss: 0.5409 - val_acc: 0.7378\n",
      "Epoch 6/10\n",
      "10513/10513 [==============================] - 3s 291us/step - loss: 0.4824 - acc: 0.7751 - val_loss: 0.5401 - val_acc: 0.7403\n",
      "Epoch 7/10\n",
      "10513/10513 [==============================] - 2s 178us/step - loss: 0.4753 - acc: 0.7810 - val_loss: 0.5741 - val_acc: 0.7179\n",
      "Epoch 8/10\n",
      "10513/10513 [==============================] - 2s 174us/step - loss: 0.4675 - acc: 0.7845 - val_loss: 0.5449 - val_acc: 0.7419\n",
      "Epoch 9/10\n",
      "10513/10513 [==============================] - 1s 134us/step - loss: 0.4646 - acc: 0.7865 - val_loss: 0.5544 - val_acc: 0.7352\n",
      "Epoch 10/10\n",
      "10513/10513 [==============================] - 1s 120us/step - loss: 0.4628 - acc: 0.7878 - val_loss: 0.5511 - val_acc: 0.7317\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f23bd9e6828>"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Train\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val) ) #, callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ...and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3509/3509 [==============================] - 0s 55us/step\n",
      "Test accuracy: 0.7372470790248\n"
     ]
    }
   ],
   "source": [
    "# 4. Evaluate\n",
    "# Accuracy\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get actual predictions for test set\n",
    "predictions = model.predict(X_test)\n",
    "rounded_predictions = [int(float(round(x[0]))) for x in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "Predicted  False  True  __all__\n",
      "Actual                         \n",
      "False       1231   505     1736\n",
      "True         417  1356     1773\n",
      "__all__     1648  1861     3509\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f234fd0a4e0>"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# Confusion Matrix\n",
    "\n",
    "confusion_matrix = ConfusionMatrix(y_test, rounded_predictions)\n",
    "print(\"Confusion matrix:\\n%s\" % confusion_matrix)\n",
    "confusion_matrix.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC:  0.8090861774544434\n"
     ]
    }
   ],
   "source": [
    "# ROC AUC\n",
    "print('ROC AUC: ', roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-589-440e5fd394b4>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-589-440e5fd394b4>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    for a in i\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Error out to stop notebook\n",
    "for a in i\n",
    "def \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Examine the predictions\n",
    "Let's look at the predictions the NN gets wrong, see if there's a pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create df with the relevant data\n",
    "test_set = df1.loc[test_rows]\n",
    "test_set['Predictions'] = predictions\n",
    "test_set['RoundPredictions'] = rounded_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimise memory --> set col types for the incoming CSV\n",
    "cds_cols = ['# ChemicalName', 'ChemicalID', 'DiseaseName', 'DiseaseID', 'DirectEvidence']\n",
    "cd_col_types = {   \n",
    "    '# ChemicalName': 'category',\n",
    "    'ChemicalID': 'category',\n",
    "    'DiseaseName': 'category',\n",
    "    'DiseaseID': 'category',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the disease and chemical names back! For the sake of eyeballing for patterns\n",
    "# Read in CTD csv, skipping the intro rows\n",
    "df_cd = pd.read_csv('../ctd-to-nt/csvs/CTD_chemicals_diseases.csv', usecols=cds_cols, dtype=cd_col_types, skiprows=27)\n",
    "df_cd = df_cd.drop(0)\n",
    "df_cd = df_cd.dropna(subset=['DirectEvidence']) # drop if it doesn't have direct evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set.DiseaseID.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['Correlation'] = test_set.Correlation.astype(np.uint8)\n",
    "print(mem_usage(test_set['RoundPredictions']))\n",
    "test_set['RoundPredictions'] = test_set.RoundPredictions.astype(np.uint8)\n",
    "print(mem_usage(test_set['RoundPredictions']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [chem_dis, df, df_d, df_c, Dvecs, Cvecs]\n",
    "del lst\n",
    "test_set = test_set.drop(['DVec', 'CVec', 'index'], axis=1) # memory intensive\n",
    "for col in ['DiseaseID', 'ChemicalID', 'DiseaseName', '# ChemicalName', 'DirectEvidence']:\n",
    "    print(col,  df_cd.columns)\n",
    "    if str(col) in df_cd.columns: print('sd') # df_cd[col] = df_cd[col].astype('category')\n",
    "    if col in test_set.columns: test_set[col] = test_set[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_usage(df_cd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge in the names\n",
    "\n",
    "# Because this weirdly requires a tonne of memory, let's optimise (for stupid terrible top-of-range dell laptop)\n",
    "# lst = [chem_dis, df, df_d, df_c, Dvecs, Cvecs]\n",
    "# del lst\n",
    "# test_set = test_set.drop(['DVec', 'CVec', 'index'], axis=1) # memory intensive\n",
    "# for col in ['DiseaseID', 'ChemicalID', 'DiseaseName', '# ChemicalName', 'DirectEvidence']:\n",
    "#     if col in df_cd.columns: df_cd[col] = df_cd[col].astype('category')\n",
    "#     if col in test_set.columns: test_set[col] = test_set[col].astype('category')\n",
    "\n",
    "test_set = pd.merge(test_set, df_cd[['DiseaseID', 'DiseaseName']], on='DiseaseID')\n",
    "test_set = pd.merge(test_set, df_cd[['# ChemicalName', 'ChemicalID']], on='ChemicalID')\n",
    "\n",
    "# weirdly these operations introduce millions of duplicate rows, so delete duplicates:\n",
    "test_set = test_set.drop_duplicates(list(set(test_set.columns.values))) #- set(['DVec','CVec'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cd.ChemicalID = df_cd.ChemicalID.astype('category')\n",
    "type(df_cd.ChemicalID[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set[['DiseaseName', '# ChemicalName', 'Correlation', 'Predictions', 'RoundPredictions']].to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import gofunction counts (for each disease and each chem). This csv was output in opa2vec.ipynb\n",
    "gofunc_counts = pd.read_csv('gofunc_counts.csv')\n",
    "test_set = pd.merge(test_set, gofunc_counts[['ChemicalID', 'gofunc']], on='ChemicalID')\n",
    "test_set = test_set.rename(columns = {'gofunc':'ChemGoFuncs'})\n",
    "test_set = pd.merge(test_set, gofunc_counts[['DiseaseID', 'gofunc']], on='DiseaseID')\n",
    "test_set = test_set.rename(columns = {'gofunc':'DisGoFuncs'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This cell is pointless - manually verifying accuracy test\n",
    "# # Round predictions to int based on threshold, run accuracy-test manually\n",
    "# predictions = model.predict(X_test)\n",
    "# threshold = predictions[:].sum()/len(predictions) # Threshold is the mean value of predictions\n",
    "# predictions = [float(round(x[0]-threshold+0.5)) for x in predictions]\n",
    "# manual_accuracy = sklearn.metrics.accuracy_score(y_test, predictions, normalize=True, sample_weight=None)\n",
    "# print(manual_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Calculate Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate out the cosine similarity and see if there's a difference between groups\n",
    "# def cosine_sim (row):\n",
    "#     return cosine_similarity(np.array(row.DVec).reshape(1, -1), np.array(row.CVec).reshape(1, -1))[0][0]\n",
    "\n",
    "# df1['cosine_sim'] = df1.apply(lambda row: cosine_sim(row), axis=1)\n",
    "\n",
    "# # Compare cosine sim of correlated and uncorrelated groups\n",
    "# print('Cosine mean with no correlation: ', df1[df1.Correlation == 1 ].cosine_sim.mean())\n",
    "# print('Cosine mean with correlation: ', df1[df1.Correlation == 0 ].cosine_sim.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save model (in two files, one for weights and one for json)\n",
    "# json_string = model.to_json()\n",
    "# model.save_weights(\"model2-0.82.h5\")\n",
    "# with open('model2-0.82.json', 'w') as outfile:\n",
    "#     json.dump(json_string, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
